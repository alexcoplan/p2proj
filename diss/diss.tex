% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[UKenglish]{isodate}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{parskip}
\usepackage{verbatim}
\usepackage{color}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lilyglyphs}
\usepackage{fontspec}
\usepackage{csquotes}
\usepackage{mathtools}
\usepackage{float} % figure positioning
\usepackage{bm} % bold symbols in mathmode

% tikz for drawing
\usepackage{tikz}
\usepackage{tikz-uml}

% pseudocode typesetting
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

% math macros
\newcommand{\set}[1]{ \left\{ #1 \right\} }
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

% music notation
\newcommand{\insharp}[0]{\sharp[raise=0.1,scale=0.8]}
\newcommand{\inflat}[0]{\flat[raise=0.1,scale=0.8]}

\usepackage[style=numeric,backend=biber]{biblatex}
\addbibresource{refs.bib}

\usepackage{docmute}   % only needed to allow inclusion of proposal.tex

\newcommand{\todo}{\textcolor{red}{\textbf{todo}~}}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\cleanlookdateon

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title

\pagestyle{empty}

\rightline{\LARGE \textbf{Alex Coplan}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{A Comparison of Statistical Models and Recurrent Neural Networks for the
Generation of Music} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
St Catharine's College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{r p{10.5cm}}
Name:               & \bf Alex Coplan                       \\
College:            & \bf St Catharine's College                     \\
Project Title:      & \bf A Comparison of Statistical Models and Recurrent
Neural Networks for the \newline Generation of Music \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2017  \\
Word Count:         & \todo\footnote{1} \\
Project Originator: & Alex Coplan \\
Supervisor:         & Matthew Ireland                    \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

The original aim of this project was to implement two models for music
generation and subsequently compare them: namely, a \emph{recurrent neural
network} and \emph{multiple viewpoint system}. The two models were
to be compared using both a listening survey involving human participants and
objective metrics of evaluation, such as information-theoretic measures
of predictive performance.

\section*{Work Completed}

All that has been completed appears in this dissertation.

\section*{Special Difficulties}

\todo
 
\newpage
\section*{Declaration}

I, Alex Coplan of St Catharine's College, being a candidate for Part II of the
Computer Science Tripos, hereby declare that this dissertation and the work
described in it are my own work, unaided except as may be specified below, and
that the dissertation does not contain material that has already been used to
any substantial extent for a comparable purpose.

\bigskip
\leftline{Signed }

\medskip
\leftline{Date }

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

Acknowledge acknowledge acknowledge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}

The modelling and automated generation of music is a central task in an approach
to understanding computational creativity. It is natural to ask whether
computers can produce music that is compelling to humans, and indeed, this
question has long been posed by researchers, with practical efforts dating back
to the mid-1950s \cite{ames1987automated}. 

The aim of this work is to implement and compare two modern techniques for
\emph{melody generation}, a task highly amenable to statistical modelling and
machine learning. 

An automated system for melody generation is motivated by end-user applications
in \emph{computer-assisted composition}, whereby the system can provide
inspiration for a human composer, either by generating entirely novel melodies
within stylistic constraints, or extemporising on or extending melodic
fragments written by the human composer. Such a system might augment the
capabilities of typical music notation software. 

Markov modelling is a simple yet effective technique for capturing the
statistics of sequential data. An obvious tool to apply to the modelling of
melody is the Markov chain \cite{ames1989markov}. However, an important
observation to make is that music has a rich underlying structure: modelling the
statistics of notes directly (the \emph{surface structure}) is insufficient to
capture the complex language of musical style. This observation motivated the
development of more sophisticated models, known as \emph{multiple viewpoint
systems} \cite{conklin1995viewpoints}.  A multiple viewpoint system exploits the
rich underlying event structure of complex languages by combining the
predictions of an ensemble of context models, each modelling a different
attribute of the event space.

Recurrent neural networks (RNNs), the natural topology of neural network for
modelling sequential data, have been widely applied to tasks such as language
modelling \cite{graves2013generating}, machine translation
\cite{sutskever2014sequence}, and indeed to the modelling of music
\cite{boulanger2012modeling}. A RNN is an end-to-end sequence learning tool which
relies on very little domain knowledge aside from the chosen input
representation. This means that a system designed for character-level language
modelling can equally be trained to model melody without changing the network
architecture. In practice, however, we shall see that slight architectural
modifications can be beneficial.

In this work, we compare the performance of multiple viewpoint systems with that
of recurrent neural networks on a task of stylistically-constrained melody
generation. In particular, we assess predictive performance, in terms of a
\emph{cross-entropy} loss function, on a corpus of melodies used in the chorale
harmonisations of J.S.\ Bach. Furthermore, we compare the sampled outputs from
each model by means of a listening survey involving human participants.

A high-level goal of this work which motivates the choice of models involved is
understanding to what extent \emph{feature engineering} impacts the
effectiveness of musical models. A recurrent neural network, at one extreme, is
a domain-independent, end-to-end learning system, capable of learning to extract
high-level features from sequential data \cite{Goodfellow-et-al-2016}. A system
of viewpoints, however, relies on its creator to use domain knowledge to
determine a set of salient features, encoded in the form of the \emph{pool} of
viewpoints made available to the system.

\section{Background}

\subsection{Music Theory}

The development and implementation of the recurrent neural network in this work
can be understood with minimal knowledge of music theory. In general, the
multiple viewpoint formalism can also be expressed independently of any domain
knowledge.  However, to understand the application of the multiple viewpoint
framework to music requires an elementary understanding of music theory.  This
section introduces the relevant terminology to enable such discussion.

We shall constrain our discussion to the context of Western classical music,
since the style which we wish to model lies within this context. A \emph{pitch}
is an abstract concept which is related to the frequency of a musical note.
Formally, if $p,q$ are pitches with corresponding frequencies $\nu_p,\nu_q$,
then if $\nu_q = 2\nu_p$, $p$ and $q$ are said to span an \emph{octave}, with
$q$ an octave above $p$. In Western classical music, each octave is divided into
twelve distinct \emph{note names} (A through G modified by accidentals
\insharp{} or \inflat{}). A note name, such as `B\inflat' (or, equivalently,
A\insharp), is typically thought of as not just a single pitch, but a
\emph{pitch class}, the set of all pitches an integer number of octaves above or
below any pitch with that note name.  Formally, given some reference frequency
$\nu_n$ for a note name $n$, the set of frequencies of the pitches in the pitch
class for $n$ is given by $\set{ 2^k \cdot \nu_n\ |\ k \in \mathbb{Z} }.$

The mapping between pitch and frequency is known as \emph{temperament}, which on
modern instruments is typically \emph{equal}, meaning that the frequency space is
equally divided among the twelve pitches in an octave. More precisely, the
$(k+1)$\textsuperscript{th} note of the chromatic scale with base frequency $\nu_0$ is
given by $2^{k/12}\cdot\nu_0$. As an aside, all recordings of
model outputs in this work were made using equal temperament. Herein
we will work with the abstraction of pitch, ignoring the underlying frequencies
of notes.

To refer to specific pitches, we make use of \emph{scientific pitch notation}
(SPN). Write $N_k$ where $N$ is a note name and $k$ is an octave number. To
ground all the definitions made thus far, we use the standard reference
frequency $\mathrm{A}_4 \triangleq 440\ \mathrm{Hz}$. An \emph{interval} refers
to the difference in pitch between two notes. Consecutive note names are said to
be a \emph{semi-tone} apart, and an interval of two semi-tones is known as a
\emph{tone}. Later in this work we shall further abstract pitch and intervals
using MIDI pitch numbering, and further concepts shall be defined as necessary.
In the MIDI tuning standard, $\mathrm{A}_4$ corresponds to pitch number $69$.

\todo talk about key/tonality? this section getting quite long \ldots

\subsection{Melody Generation}

\todo Reference literature to explain why this is difficult task. Background in
Pearce's thesis?

\section{Work Accomplished}

A complete framework for implementing Multiple Viewpoint Systems in \verb!C++!
has been implemented. The formalism as described by Conklin et al.\
\cite{conklin1995viewpoints} has been fully implemented.  At the core of the
framework, the \emph{prediction by partial match} (PPM) algorithm
\cite{cleary1984ppm} was implemented for smoothing variable-order context
models, which can be parameterised over arbitrary types using \verb!C++!
templates. The distributions predicted by these context models can be combined
using either a \emph{weighted arithmetic} scheme (as per
\cite{conklin1995viewpoints}) or \emph{weighted geometric} scheme, the latter
found to be more effective by both Pearce \cite{pearce2005construction} and
Whorley \cite{whorley2013phd}.

Abstractions were made such that, using these primitives, \emph{primitive},
\emph{derived}, \emph{linked}, and \emph{triply-linked} viewpoints can be
instantiated over arbitrary combinations of types. Both a \emph{long-term}
model, for capturing regularity throughout the corpus, and a \emph{short-term}
model, for capturing the regularity within a composition, were implemented.
Sampling by random walk was implemented for melody generation.  Finally, an
automatic viewpoint selection algorithm in the form of \emph{forward step-wise
selection} \cite{whorley2013phd} was implemented.

Code for corpus preparation and analysis was written in Python using the
\texttt{music21} package. This code was used by both the MVS and RNN
implementations.

A multi-layer LSTM recurrent neural network was implemented in Python using
TensorFlow \cite{abadi2016tensorflow}. The network includes \emph{dropout} for
regularisation as per Zaremba et al.\ \cite{zaremba2014recurrent}. A decoupled
``front-end'' to the RNN was implemented which allows the same internal model to
be used for character-level language modelling and music modelling. Random walk
sampling was implemented for generation. In addition, modifications to the
conventional sequence-prediction RNN architecture were made by feeding the
network additional \emph{global} musical information at each timestep, which was
found to improve performance significantly.

Finally, a website for human evaluation was developed. This will be discussed in
detail in Chapter~\ref{chap:eval}.

\section{Related Work}

\subsection{Multiple Viewpoints}

The general method of multiple viewpoints was first applied to music in 1986 by
Ebcioğlu in a rule-based system for chorale harmonisation
\cite{ebcioglu1986expert}.  This system used hand-crafted rules written in
first-order logic which were expressed in terms of different viewpoints of the
music. This system did not make use of Markov models, the main primitive
underlying \emph{statistical} viewpoint systems.

In 1988, Conklin and Cleary \cite{conklin1988modelling} applied multiple
viewpoints with underlying probabilistic Markov models to modelling Gregorian
chant and simple two-part polyphony. The authors make use of the
\emph{prediction by partial match} (PPM) algorithm \cite{cleary1984ppm} for
smoothing variable-order Markov models, but the escape method used is not
specified.  An ad-hoc, unweighted method is used for combining the predictions
of viewpoints.

Conklin and Witten went on to develop a formalism around multiple viewpoints in
a key 1995 paper \cite{conklin1995viewpoints}. The authors justify discounting
the
\emph{knowledge engineering} approach:

\begin{displayquote}
``There are too many exceptions to any
logical system of musical description, and it will be difficult to ensure the
completeness of an intuited theory. The system will always exclude some valid
pieces. The generations of a theory are bound to reflect the biases of an
engineer; this is the only way they might be called creative.''
\end{displayquote}

With similar reasoning, we shall only consider models which learn from data.  At
this point in time, the convention in the literature became to use ``multiple
viewpoint system'' to refer to \emph{statistical} viewpoint systems with
underlying context models: a convention we shall also adopt.

Many of the ideas in this paper were first published in Conklin's 1990 thesis
\cite{conklin1990prediction}. The notion of separate \emph{long-term} and
\emph{short-term} models in a viewpoint system was introduced in this work.
Conklin also details a principled method for distribution combination, namely
\emph{entropy-weighted arithmetic combination}. 

Among the contributions from his 2005 thesis \cite{pearce2005construction},
Pearce introduces a \emph{geometric} viewpoint combination technique, shown to
outperform its arithmetic counterpart, as well as a method for automatic
viewpoint selection, thereby greatly reducing the selection bias in multiple
viewpoint systems. The work concerns the modelling of melody.

More recent work by Whorley~\cite{whorley2013phd} uses multiple viewpoint
systems to model four-part harmony in Bach chorales.

\subsection{Recurrent Neural Networks}

\todo Pre-LSTM applications to music, invention of LSTM, then post-LSTM
applications to music.

\section{Structure}

\todo

\chapter{Preparation}

\todo mention \textbf{starting point} here.

\section{Deliverables}

Recalling the proposal, the success criteria for the project include the
following deliverables:
\begin{itemize}
  \item A program or tool for importing the corpus in the desired format.
  \item A multiple viewpoint system (MVS) capable of generating melody.
  \item A recurrent neural network (RNN) capable of generating melody.
  \item Quantitative and human evaluation.
\end{itemize}

This chapter concerns the ideas behind the first three of these, along with
a discussion of the research and decisions made prior to implementation.
Chapter~\ref{chap:eval} will discuss the design and implementation of the
human evaluation survey in detail, as well as quantitative evaluation.

\section{Corpus Preparation}

The chosen corpus was a set of chorale melodies used in the harmonisations of
J.S.\ Bach. The Bach Chorales are a corpus of great interest in the
computational modelling of music: the style is unified, yet with significant
inter-opus variance; it is not too complex, yet not overly simplistic.

Initial inspection of relevant corpora and their formats indicated that writing
a parser for an established music notation format such as MusicXML would take a
considerable amount of time and offer limited flexibility. After further
research, it was decided that the Python package \texttt{music21} would be
utilised. \texttt{music21} is an open-source toolkit for computational
musicology. This package was especially useful for the task of corpus
preparation due to the following features:
\begin{itemize}
  \item Built-in corpora including the Bach chorales in MusicXML format.
  \item Parser for various music notation formats, including MusicXML.
  \item High-level features for manipulating musical data.
\end{itemize}

As will be discussed later, the flexibility offered by \texttt{music21} became
of great use later in the project as the requirements in corpus preparation and
analysis changed.

An internal format for melody based on a representation used commonly in the
multiple viewpoint system literature (e.g.\ \cite{conklin1995viewpoints}) was
decided on. It was decided that musical time should be quantised with a quantum
of $1/16$\textsuperscript{th} notes (semiquavers). 

This representation was to be used internally for both the MVS and
RNN implementations. A note $n$ is represented as a tuple $(p,o,d) \in
\mathbb{N}^3$ where $p$ is the MIDI pitch number of $n$, $o$ is the onset 
time, and $d$ is the duration. A melody is then a list of such tuples. Note that
rests are not explicitly represented in this encoding.

\section{Multiple Viewpoint Systems}

In this section, the key theory, algorithms, and data structures underpinning
multiple viewpoint systems are introduced. The notation and formalism is as per
Conklin and Witten \cite{conklin1995viewpoints}. We start by defining some
notation which will be used throughout this section.

\begin{itemize}[itemsep=0mm]
  \item If $\tau$ is a type, then $[\tau]$ is the \emph{syntactic domain} of
    that type: all elements of type $\tau$.   
  \item $S^*$ denotes the set of all sequences drawn from a set $S$.
  \item $e_i^j$ abbreviates the sequence $(e_i,e_{i+1},\ldots,e_{j-1},e_j)$ and
    $()$ the empty sequence.
  \item $s :: e$ denotes the sequence $s$ with event $e$ appended.
  \item $\zeta$ denotes the \emph{event space}, the set of all possible events
    that can occur in sequences of interest. For example, a very simple event
    space for melody might be:
    $$ [\mathrm{pitch}] \times [\mathrm{duration}] \times [\mathrm{onset}]. $$
\end{itemize}

\subsection{Context Models}\label{sec:ctx-model-prep}

The central primitive underlying a multiple viewpoint system is the
\emph{context model}. In general, a context model over some type $\tau$ is
a data structure storing sequences in $[\tau]^*$, together with an inference
procedure predicting distributions over $[\tau]$.

While first-order Markov chains make predictions based on just one timestep of
context, an $n$\textsuperscript{th} order Markov chain makes predictions based
on $n$ elements of context. For notational convenience, and for consistency with
the literature, we generally refer to the \emph{history} $h = n+1$ of an
$n$\textsuperscript{th} order Markov model. Such a model stores and models
$h$-grams.

The context models we consider are variable-order Markov models with some
maximum history $\hbar$.  Such a model can be thought of as a combination of
Markov models with history $1,2,\ldots,\hbar$. 

A known problem with high-order Markov models is their space complexity. A naïve
tabular approach uses $\Theta(|[\tau]|^h)$ space.  In practice, such high-order
models are \emph{sparse}: most $h$-grams are never seen in the training data. A
data structure which exploits this sparsity is the \emph{suffix tree} or
\emph{trie}.  In such a structure, the nodes are events, each with an associated
count, and a path from the root corresponds uniquely to a particular $h$-gram.
Figure~\ref{fig:dur-trie} illustrates this data structure, in this case the trie
supporting a context model over musical durations. 

\begin{figure}[H]
\centering
\includegraphics[width=455pt]{figs/duration_vp.pdf}
\caption{Trie for a context model over duration with $\hbar = 3$}
\label{fig:dur-trie}
\end{figure}

The algorithm for extracting $h$-grams for a training sequence $e_1^k$ works by
passing a sliding window of size $\hbar$ across the input sequence and
generating subsequences. This sliding window procedure is detailed in
Algorithm~\ref{alg:sliding-window} and illustrated in
Figure~\ref{fig:hgram-extract} for an input sequence $e_1^5$.

\begin{algorithm}[H]
  \caption{Sliding window algorithm for $h$-gram extraction}
  \label{alg:sliding-window}
  \begin{algorithmic}[1]
    \Function{learn-sequence}{$e_1^k \in [\tau]^*$}
      \For{$i : 1 \rightarrow k$}
        \State \Call{add-or-increment}{()}
        \For{$j : \min(1,i - \hbar) \rightarrow i$} \Call{add-or-increment}{$e_j^i$}
        \EndFor
      \EndFor
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{figure}[H]
\centering
\includegraphics[width=300pt]{figs/sliding_window_tmp.jpg}
\caption{Illustration of the $h$-gram extraction algorithm with $\hbar = 3$}
\label{fig:hgram-extract}
\end{figure}

The subroutine \Call{add-or-increment}{$e_i^j$} looks up $e_i^j$ in the trie. If
it is found, then its count is incremented. Otherwise, $e_i^j$ is inserted into
the trie, the resulting node initialised with a count of unity. Note that $()$,
the empty sequence, corresponds to the root node in the trie.

Once we have constructed the trie for a context model, we need an algorithm to
perform \emph{inference}. In particular, we want to compute $\mathbb{P}(e' |
e_1^k)$ for some next event $e' \in [\tau]$ and input context $e_1^k \in
[\tau]^*$.

The maximum likelihood solution to this problem is simply to use the relative
frequency of the $h$-gram of interest:
\begin{equation}
  \mathbb{P}(e'|e_1^k) = \frac{ C(e_1^k::e') }{ \sum_{e''} C(e_1^k::e'') }
  \label{eq:ctx-max-like}
\end{equation}
where $C(e_i^j)$ denotes the count associated with $e_i^j$ in the trie, or $0$
if such a node does not exist. Note that all sequences longer than $\hbar$ (and
contexts longer than $\hbar-1$) are implicitly ignored, so $\forall j \in
\mathbb{N}.\ C(e_{k-\hbar-j}^k) = C(e_{k-\hbar+1}^k)$ and
$\mathbb{P}(e'|e_{k-\hbar+1-j}^k) = \mathbb{P}(e'|e_{k-\hbar+2}^k)$.

There are two main problems with this solution. The first is that we want our
models to be \emph{non-exclusive}; that is, $\forall e' \in [\tau].\
\mathbb{P}(e' | e_1^k) > 0$. An \emph{exclusive} model would limit the scope for
creativity in the system, and moreover, from a practical standpoint, evaluation
metrics such as \emph{cross-entropy} require calculating log-probabilities: we
therefore want to avoid zero probabilities. This model, however, fails to
guarantee non-exclusivity.

The second problem with this solution is that it does not deal with \emph{novel
contexts}. In particular, suppose we have not seen the context $e_1^k$. Then,
clearly, the denominator of (\ref{eq:ctx-max-like}) will be zero, which is
equally problematic.

An algorithm which attends to both of these issues is \emph{prediction by
partial match} (PPM) \cite{cleary1984ppm}. The central idea behind PPM is that,
instead of distributing the probability mass entirely among the exact context
matches, as per (\ref{eq:ctx-max-like}), we instead reserve some amount of the
probability mass known as the \emph{escape probability}. The escape probability
is then distributed recursively among a lower-order model. In this sense, PPM
is a form of \emph{backoff smoothing}.

Backoff smoothing algorithms are typically formulated recursively, as per
(\ref{eq:ppm-general}).
\begin{equation}\label{eq:ppm-general}
  \mathbb{P}(e' | e_1^k) = \begin{cases}
  \alpha(e'|e_1^k) & C(e' | e_1^k) > 0 \\
\gamma(e_1^k) \cdot \mathbb{P}(e' | e_{(k - \hbar) + 2}^k) & \text{otherwise}
\end{cases} 
\end{equation} 

If no match is found for any model, the recursion bottoms out with a uniform
distribution. The choice of \emph{escape probability} $\gamma(\cdot)$ is known
as the \emph{escape method}: established methods include A, B, C, D, and AX
\cite{pearce2004improved}. PPM A, the method implemented in this work, uses the
following $\alpha$ and $\gamma$ functions:
\begin{align}
  \label{eq:ppm-a}
  \alpha(e' | e_1^k) &= \frac{ C(e' | e_1^k) }{ 1 + \sum_{e''} C(e'' | e_1^k) }
  \\
  \gamma(e_1^k) &= \frac{ 1 }{ 1 + \sum_{e'} C(e' | e_1^k) }
  \label{eq:ppm-escape}
\end{align}

This recursive formulation can be somewhat misleading, as the notation implies
that (\ref{eq:ppm-general}) gives rise to properly normalised probability
distributions.  This is in fact not the case. Consider performing PPM with
$\hbar = 2$ over an alphabet $\Sigma = \set{\alpha,\beta,\gamma}$, and suppose
we learn the sequence $(\alpha,\beta,\gamma,\alpha,\alpha,\beta)$.
Figure~\ref{fig:bad-ppm-trie} illustrates the situation, and includes virtual
$\epsilon$-nodes to represent the escape mass.

\begin{figure}[H]
\centering
\includegraphics[width=400pt]{figs/problematic_trie_tmp.jpg}
\caption{Trie demonstrating lack of normalisation in standard PPM.}
\label{fig:bad-ppm-trie}
\end{figure}

The first problem is the escape node at layer $h = 1$. According to
(\ref{eq:ppm-escape}) it should have an effective count of $1$. However, this
yields:
$$ \sum_{e \in \Sigma} \mathbb{P}(e) = \frac{3}{7} + \frac{2}{7} + \frac{1}{7} =
\frac{6}{7} \neq 1. $$
The observation to make here is that if \emph{all} symbols in our alphabet have
a non-zero count for any given context, then the escape probability is
redundant. Suppose, then, we set the effective count of this $\epsilon$-node to
zero. Consider the distribution $\mathbb{P}(e|\alpha)$. We can trivially compute
$\mathbb{P}(\alpha|\alpha) = 1/4$ and $\mathbb{P}(\beta|\alpha) = 1/2$, and for
$\gamma$ we escape to find $\mathbb{P}(\gamma|\alpha) = 1/4 \cdot
\mathbb{P}(\gamma)$. However, $\mathbb{P}(\gamma) < 1$, so
$\mathbb{P}(e|\alpha)$ is not normalised. The problem here is that the escape
mass for the node labelled $\alpha\epsilon$ is distributed among all the symbols
at layer $h = 1$ in the trie, when in reality we can only ever escape to
$\gamma$.

There are two typical solutions for this lack of normalisation in PPM: in
earlier work, authors often simply compute an improper distribution and later
normalise it (e.g.\ \cite{conklin1990prediction}). A more sophisticated and less
wasteful solution which reasons about the possible $h$-grams that can be escaped
to is known as PPM with \emph{exclusion} \cite{pearce2004improved}, detailed in
Algorithm~\ref{alg:ppm-a}.

The key to exclusion in this algorithm is the \textit{Dead} set that is
maintained throughout the recursion. This set should be initialised to
$\varnothing$ and later populated with those events that we \emph{could} have
already matched at a higher-order model and therefore no longer need to consider
at lower-order models. To compute $\mathbb{P}(e' | e_1^k)$ we call
\Call{ppm-a}{$e_1^k$, $e'$, $1$, $\varnothing$}.

\begin{algorithm}[H]
  \caption{PPM A with exclusion}
  \label{alg:ppm-a}
  \begin{algorithmic}[1]
    \Function{ppm-a}{$e_1^k \in [\tau]^*$, $e' \in [\tau]$, $i_{\mathrm{begin}}$, 
    $\textit{Dead} \subseteq [\tau]$}
      \State $\textit{Alive} \gets [\tau] \setminus \textit{Dead}$
      \If{$i_{\mathrm{begin}} > k$}
      \State \Return $1 / |\textit{Alive}|$ \Comment{Base case: uniform
      distribution}
      \EndIf
      \State $i \gets i_{\mathrm{begin}}$
      \For{$i : i_{\mathrm{begin}} \rightarrow k$}
      \If{$C(e_i^k) > 0$} \textbf{break}
      \Comment{Find longest context match}
        \EndIf       
      \EndFor 
      \State $\textit{Known} \gets \set{ e'' \in [\tau]\ |\ C(e_i^k :: e'') > 0 }$
      \State $\textit{Novel} \gets Alive \setminus Known$ \Comment{All
      events we could escape for}
      \State $s \gets \sum_{e'' \in \textit{Alive}} C(e_i^k :: e'')$ 
      \If{$\textit{Novel} = \varnothing$}
        \State \Return $C(e_i^k :: e') / s$ \Comment{No possible need for escape}
      \EndIf
      \If{$e' \in \textit{Known}$}
      \State \Return $C(e_i^k::e') / (1 + s)$ \Comment{Matched $e_i^k::e'$}
      \EndIf
      \State \Return \Call{ppm-a}{$e_1^k$, $e'$, $i+1$, $Dead \cup Known$}$ / (1
      + s)$ \Comment{Escape recursively}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\subsection{Viewpoints}\label{sec:mvs-formalism}

One possible approach to modelling music would be to form a context model over
the entire event space $\zeta$.  Recall that $\zeta = [\tau_1] \times [\tau_2]
\times \cdots \times [\tau_n]$ is the cartesian product of many types. Aside
from being computationally expensive, this approach is severely limited in its
predictive power, since it requires an \emph{exact match} of the musical
context to make useful predictions: that is, there is no ability of the system
to \emph{generalise} to unseen contexts.

An obvious approach to allow the system to generalise would be to model some or
all of the $\tau_i$ \emph{independently}. To see why this might be useful,
consider that while there is certainly some correlation between e.g.\ pitch and
duration, much of the rhythmic regularity in a particular corpus is independent
of the pitch of the notes: thus, it is likely to be fruitful to model duration
independently of any of the other musical attributes.

Further, complex languages such as music can be viewed from many abstract
interpretations. One such abstract domain is the \emph{melodic
interval}: it is clear (\todo is it?) that intervalic patterns in music capture
regularity that is \emph{invariant} to the absolute pitch. To take this
abstraction to an extreme, we can consider the melodic \emph{contour}, which we
might represent with the domain $\set{-1,0,1}$, where the elements indicate a
decrease in pitch, stationary pitch, and increase in pitch respectively.
Modelling the regularity in such abstract domains and transforming the
predictions back into predictions over the concrete $\tau_i$ might therefore
further allow the system to generalise.  

\emph{Viewpoints} allow us to model music from these different points of view:
namely, we can model the individual $\tau_i$, as well as abstract
interpretations and combinations thereof. We now proceed to set up the formalism
of multiple viewpoints.

First, allow $\tau$ to range over types other than just the \emph{basic types}
that make up the component types of $\zeta$. We call these non-surface types
\emph{derived types}: the abstract interpretations derived from the surface
$\tau_i$. 

\textbf{Definition}. A \emph{viewpoint} modelling a type $\tau$ is:
\begin{enumerate}[label=\arabic*., itemsep=0mm]
  \item a partial function $\Psi_\tau : \zeta^* \rightharpoonup [\tau]$,
    together with
  \item a context model of sequences in $[\tau]^*$.
\end{enumerate}

The function $\Psi_\tau$ for a type $\tau$ is known as the \emph{projection}
function for $\tau$: it projects out the last element of type $\tau$ from some
surface event stream $e_1^k \in \zeta^*$, if such an element exists. Note that,
for convenience, we shall frequently refer to viewpoints simply by the type they
model. 

Viewpoints, as we have defined them, can clearly model individual surface and
derived types. While this is useful, we also want to be able to capture the
correlation between attributes.

\textbf{Definition}. A \emph{product type} $\tau_1 \otimes \cdots \otimes
\tau_n$ between $n$ types $\tau_1, \ldots, \tau_n$ is itself a type $\tau$ with
$[\tau] = [\tau_1] \times \cdots \times [\tau_n]$. 

For a product type $\tau = \tau_1 \otimes \cdots \otimes \tau_n$:
$$ \Psi_\tau(e_1^k) \triangleq
\begin{cases}
  \langle\Psi_{\tau_1}(e_1^k), \ldots, \Psi_{\tau_n}(e_1^k)\rangle & \forall i
  \in \set{1,\ldots,n}.\
  \Psi_{\tau_i}(e_1^k)\downarrow \\
  \bot & \text{otherwise.}
\end{cases}
$$

Viewpoints over product types are known as \emph{linked viewpoints}. 

Note that one can also make use of \emph{threaded viewpoints}: those defined
only at certain fixed intervals in a sequence. However, these are considered out
of scope for this work.

In order to train a viewpoint over type $\tau$, we need to provide the
underlying context model with sequences in $[\tau]^*$. To obtain such sequences
from surface events, we simply iterate the projection function $\Psi_\tau$ to
give a function $\Phi_\tau : \zeta^* \rightarrow [\tau]^*$ which we call the
\emph{lifting} function, defined as follows:
\begin{align*}
  \Phi_\tau(()) &\triangleq () \\
  \Phi_\tau(e_1^k) &\triangleq \begin{cases}
    \Phi_\tau(e_1^{k-1})::\Psi_\tau(e_1^k) & \Psi_\tau(e_1^k)\downarrow \\
    \Phi_\tau(e_1^{k-1}) & \text{otherwise.}
  \end{cases}
\end{align*}

In practice, we shall see that it is typically easier to implement $\Phi_\tau$
directly. However, formally, it is cleaner to specify $\Psi_\tau$ to define a
particular viewpoint.

Now consider a viewpoint over a type $\tau$ derived from some surface type
$\tau'$. This viewpoint will predict distributions over the abstract type
$\tau$, but we actually need a distribution over the concrete $\tau'$. 

The process of transforming a distribution from the abstract domain to the
concrete is given a limited treatment in the literature. Whorley
\cite{whorley2013phd} notes that this is effectively ``using the partial
function $\Psi_\tau$ in reverse on each of the viewpoint elements''. We argue
that the surface context is also needed to perform this task, and shall refer to
this process as \emph{reification}, specified by a partial function $\rho :
\zeta^* \times \mathrm{dist}(\tau) \rightharpoonup \mathrm{dist}(\tau')$, where
$\mathrm{dist}(\tau)$ denotes the set of probability distributions over a type
$\tau$. This shall be discussed further in Chapter~\ref{chap:impl}.

\subsection{Combining Viewpoint Predictions}\label{sec:vp-comb}

Recall that, given some musical context $e_1^k \in \zeta^*$, the goal is to
predict the next event $e_{k+1} \in \zeta$ where $\zeta = [\tau_1] \times \cdots
\times [\tau_n]$.  A collection of viewpoints that performs this task is known
as a \emph{multiple viewpoint system} (MVS). 

A MVS decomposes this task by predicting $e_{k+1}$ componentwise. Out of all the
viewpoints in the system, consider just those viewpoints capable of predicting
some $\tau_i$. At each timestep, given the context $e_1^k$, $N$ of these
viewpoints will \emph{activate}, meaning they predict a distribution for
$\tau_i$. This section concerns techniques for the combination of $N$ such
distributions to form an overall prediction. In particular, we consider
\emph{weighted} schemes as these are widely used in the literature and known to
perform better than unweighted schemes \cite{pearce2004combining}.

The premise of weighted schemes for viewpoint combination is that viewpoints
that are more `confident' in their predictions should be given higher weights.
Since the Shannon entropy of a viewpoint's distribution is a metric of overall
uncertainty, we use weights that are monotonically non-increasing as a function
of distribution entropy.

Suppose we have $N$ distributions over a type $\tau$. Suppose further that
there is some canonical ordering of $[\tau]$, such that we can pick out the
$j$\textsuperscript{th} member. Now, let $p_i(j)$ denote the probability
assigned to $j$ by the $i$\textsuperscript{th} of the $N$ distributions. We want
to combine these predictions to produce some overall distribution with
probabilities $p(j)$.

\begin{figure}[H]
\centering
\includegraphics[width=400pt]{figs/dist_comb.pdf}
\caption{Plots illustrating effect of distribution combination schemes ($b = 2$)}
\label{fig:dist-comb-plot}
\end{figure}

A general weighted arithmetic scheme combines the predictions as follows:
$$
  p(j) = \frac{ \sum_{i = 1}^N w_i p_i(j) }{ \sum_{i = 1}^N w_i }.
$$

The Shannon entropy of the $i$\textsuperscript{th} distribution is given by:
$$ H(i) = - \sum_{j = 1}^{|[\tau]|} p_i(j) \log_2 p_i(j) $$
with maximum value $H_{\mathrm{max}} = \log_2{ |[\tau]| }$. Now define the
\emph{normalised entropy} $\hat{H}(i)$ as:
$$ \hat{H}(i) \triangleq \begin{cases}
  H(i)/H_{\mathrm{max}} & H_{\mathrm{max}} > 0 \\
  1 & \text{otherwise.}
\end{cases} $$

In previous work this quantity has been called the \emph{relative entropy};
here, we use \emph{normalised entropy} so as to avoid confusion with the
Kullback-Leibler distance, which is also known by this name.

Finally, so that we can favour distributions with lower $\hat{H}$, we introduce
an exponential bias $b \in \mathbb{R}_0^+$:
$$ w_i = \hat{H}(i)^{-b}. $$

Note that we do not restrict $b$ to integer values, as is commonly done in the
literature \cite{whorley2013phd}. Pearce \cite{pearce2004improved} introduces a
new method for combining viewpoint predictions: namely, a \emph{weighted
geometric mean}:
$$ p(j) = \frac{1}{Z} \left( \prod_{i = 1}^N p_i(j)^{w_i} \right)^{ \frac{1}{
\sum_{i = 1}^N w_i }} $$

where $Z$ is a normalisation constant, and the $w_i$ are calculated as before.

Figure~\ref{fig:dist-comb-plot} shows the effect of using these two schemes on
some example distributions. It can be seen that a very low probability
prediction from one distribution has considerably more bearing with the
geometric scheme. Note also that when combining distributions $C$ and $D$, since
$D$ has much lower entropy, it has considerably more effect on the overall
result under both schemes.

\subsection{MVS Architecture}

\begin{figure}[H]
\centering
\includegraphics[width=250pt]{figs/mvs_arch_tmp.jpg}
\caption{Typical MVS architecture}
\label{fig:mvs-arch}
\end{figure}

For successful sequence prediction, it is necessary to capture both
intra-sequence and inter-sequence regularity: that is, the \emph{short-term}
effects \emph{local} to a particular sequence, and the \emph{long-term} effects
common through the entire corpus.  This is especially true of music, and the
chorales in particular, where entire melodic fragments are often re-used within
a given chorale melody.

In a conventional MVS architecture, this is achieved explicitly by using
entirely separate \emph{long-term} and \emph{short-term} models. The long-term
model is trained on the entire corpus, and the short-term model just on the
sequence being predicted or generated. Figure~\ref{fig:mvs-arch} illustrates
this architecture: viewpoint combination is indicated by $\oplus$, and is
performed as per Section~\ref{sec:vp-comb}.

In order to keep the number of hyperparameters under control, the following
architectural constraints are often enforced (and indeed, will be in this work):
\begin{itemize}
  \item The same set of viewpoints $\set{\tau_1, \ldots, \tau_N}$ is used in both
    the long-term and short-term model. 
  \item The viewpoints in the long-term model all have the same order bound
    $h_l$. Similarly, those in the short-term model all have a (possibly
    distinct) order bound $h_s$.
  \item The viewpoint combination within each of these two models is performed
    using the same bias parameter $b_{\mathrm{int}}$ and the combination of the
    short-term and long-term predictions using a separate bias parameter
    $b_{\mathrm{ext}}$.
\end{itemize}

\section{Neural Networks}

Early connectionist approaches to modelling sequential data adapt conventional
feedforward networks to the task by using a finite \emph{windowed} context of
size $N$ and training the network to predict the $N$ events that follow
\cite{todd1989connectionist}. Figure~\ref{fig:windowed-nn} illustrates this
approach.

\begin{figure}[H]
\centering
\includegraphics[width=140pt]{figs/windowed_nn_tmp.jpg}
\caption{Windowed feedforward neural network}
\label{fig:windowed-nn}
\end{figure}

Observe that, in such an architecture, any common effects between e.g.\ $e_1$
and $e_2$ must be learnt entirely separately from the effects between $e_2$ and
$e_3$, even though there may be a considerable amount of time-invariant
regularity between consecutive events in the sequences of interest. Similarly,
when predicting the output $e_{N+1}^{2N}$, the network will have to learn how to
predict each of these events independently from each other (that is, without
time invariance).  

It can be seen therefore that such networks are very \emph{inefficient} sequence
learners: with a large $N$, vast amounts of training data will be needed and
many parameters to successfully capture any time-invariant effects. Moreover,
with a small $N$ the network will clearly fail to capture any long-term effects.

One solution to this problem is \emph{parameter sharing}, and is central to the
sequence-learning ability of \emph{recurrent neural networks} (RNNs). To quote
the recent text of Goodfellow et al.\ \cite{Goodfellow-et-al-2016}:
\begin{displayquote}
  ``If we had separate parameters for each value of the time index, we could not
  generalise to sequence lengths not seen during training, nor share statistical
  strength across different sequence lengths and across diﬀerent positions in
  time.''
\end{displayquote}

\subsection{Recurrent Neural Networks}\label{sec:rnn-intro}

Recurrent neural networks can be formulated as a dynamical system with a state
evolving over time. Given an initial state $\vect{h}_0$ and input data
$\vect{x}_t \in \mathbb{R}^m$, the hidden state $\vect{h}_t \in \mathbb{R}^n$ of
a RNN evolves as per (\ref{eq:dyn-sys}) for $t \in \mathbb{N}$.
\begin{equation}
  \bm{h}_t = f(\vect{h}_{t-1}, \vect{x}_t; \vect{\theta})
  \label{eq:dyn-sys}
\end{equation} 

The output
$\vect{o}_t$ may then be a function of the hidden state $\vect{h}_t$, or indeed
a function of both the state $\vect{h}_t$ and input vector $\vect{x}_t$. The key
observation to make from (\ref{eq:dyn-sys}) is that the same parameters
$\vect{\theta}$ are used at each iteration of $f$. 

We can usefully represent such a network as a \emph{computational graph}. Figure
\todo uses this representation, and illustrates the operation of
\emph{unfolding} a recurrence in a computational graph.

\todo Graph unfolding figure here.

In a basic RNN, the choice of $f$ is a simple neural network unit: a
nonlinearity (such as the hyperbolic tangent or logistic sigmoid function)
applied to an affine transformation \cite{zaremba2014recurrent}. Using
$[\vect{u},\vect{v}] \in \mathbb{R}^{m+n}$ to denote the concatenation of
vectors $\vect{u} \in \mathbb{R}^m$ and $\vect{v} \in \mathbb{R}^n$: 
$$f(\vect{h}_{t-1}, \vect{x}_t) = \sigma(\vect{W}[\vect{h}_{t-1}, 
\vect{x}_t] + \vect{b})$$ 
where $\vect{W} \in \mathbb{R}^{(m+n) \times n}$, and $\vect{b} \in
\mathbb{R}^n$. 

The next section explains how we can train recurrent networks in general, and
then proceeds to explore some of the problems due to this simple choice of $f$.

\subsection{Training Recurrent Networks}

The algorithm typically used for training a RNN, known as \emph{backpropagation
through time} (BPTT), applies the standard backpropagation algorithm to the
unfolded computational graph for our RNN. That is, we calculate the derivative
of the cost function with respect to the weights in our network \emph{at each
timestep}. Since the parameters are shared across time, we then sum the gradient
contributions from each timestep and update the weights accordingly by gradient
descent.

Note that training such a network is expensive: supposing we unfold the graph up
to $T$ timesteps, we have to compute the entire forward pass through the network
in $\Theta(T)$ time before gradients can be calculated. Moreover, this cannot be
parallelised, since the computation at timestep $t$ depends on
$\vect{h}_{t-1}$, the result of the computation at the previous timestep.

Even in a recurrent network that is \emph{spatially} shallow (i.e.\ with a
single layer at each timestep), unfolding the computational graph results in a
graph that is very deep: the network itself is \emph{temporally deep}. With the
choice of $f$ in a basic RNN, the repeated composition of an affine
transformation and nonlinearity can exhibit highly nonlinear behaviour. While
this can be desirable, granting the network much expressive power, training such
networks can be problematic. 

\todo Figure illustrating exploding gradients: generate this with a quick numpy
script.

One such problem in deep networks is that the high-dimensional surface of our
loss function with respect to the weights can exhibit sharp ``cliffs'' where the
gradient is very large \cite{Goodfellow-et-al-2016}: this is known as the
\emph{exploding gradient problem}. The negative effects of this problem on
training can, however, largely be mitigated by \emph{clipping} the norm of the
gradient to some maximum value during training.

\subsubsection{Long-term dependencies}

A more insidious problem with basic recurrent networks is the \emph{vanishing
gradient problem}. This is caused not only by the network being very deep, but
in particular by the repeated application of $f$ with the same parameters at
each timestep.

Here, we follow the illustrative treatment of Goodfellow et al.\
\cite{Goodfellow-et-al-2016} and refer to Hochreiter and Schmidhuber
\cite{hochreiter1997long} for a deeper treatment. This argument makes several
simplifying assumptions, but clearly illustrates how the problem might arise in
a simpler setting.

Suppose we simplify the function composition in a neural network to simply apply
matrix multiplication (with no inputs, biases, or nonlinearities). The
recurrence relation: 
$$ \vect{h}_t = \vect{W}^\top \vect{h}_{t-1} $$ 
describes the evolution of such a network. A straightforward induction shows:
$$ \vect{h}_t = (\vect{W}^t)^\top\vect{h}_0 $$
and, supposing that $\vect{W}$ has an eigendecomposition of the form:
$$ \vect{W} = \vect{Q}\vect{\Lambda}\vect{Q}^\top $$
with orthogonal $\vect{Q}$ (i.e.\ $\vect{Q}\vect{Q}^\top = \vect{Q}^\top\vect{Q}
= \vect{I}$), the recurrence can be further simplified to:
$$ \vect{h}_t = (\vect{Q}^\top \vect{\Lambda}^t \vect{Q}) \vect{h}_0. $$
As $t \rightarrow \infty$, any eigenvalues with magnitude less than $1$ will
vanish, and those with magnitude greater than $1$ will explode. The problem of
vanishing and exploding gradients comes from the fact that gradients through the
graph of a such a (simplified) network are \emph{also} scaled according to
$\vect{\Lambda}^t$ \cite{Goodfellow-et-al-2016}.

Complex languages such as music require a good predictor to learn long-term
dependencies in sequences. However, if the gradient vanishes in this manner as
we calculate gradients further back in time, then gradient descent will
experience an exponential slow-up in learning such dependencies. Eventually,
we will also encounter numerical problems with gradients of sufficiently small
magnitude. For this reason, learning long-term dependencies with basic RNNs is
considered intractable.

\subsection{Long Short-Term Memory}

In 1997, Hochreiter and Schmidhuber discovered a radically different RNN
architecture known as \emph{long short-term memory} (LSTM)
\cite{hochreiter1997long}. Despite the superficial complexity of this
architecture, it is based on a simple fundamental idea which is used to achieve
constant gradient flow through time, thereby enabling LSTM networks to learn
long-term dependencies over many timesteps. The idea is to maintain additional
state known as the \emph{cell state} which flows through the network with only
minor \emph{linear}, \emph{pointwise} interactions, protected by structures
known as gates. 

There are in fact many variants of LSTM: we largely follow Zaremba et al.\
\cite{zaremba2014recurrent} in both notational conventions and LSTM design.
Before introducing LSTMs in detail, we shall first define some notation and
introduce the notion of deep recurrent networks in general.

In a deep recurrent network, information is not only processed horizontally
through time, but also vertically through $L$ hidden layers ($L > 1$). We use a
homogeneous state representation with all states in $\mathbb{R}^n$, including
all input and output vectors. 

Let subscripts denote timesteps and superscripts denote layers so that
$\vect{h}_t^l \in \mathbb{R}^n$ denotes the state at time $t$ in layer $l$. In
order to represent input events $x_t$ drawn from an alphabet $\Sigma$ as vectors
in $\mathbb{R}^n$, we use an embedding $E : \Sigma \rightarrow \mathbb{R}^n$
which is either learned through pre-training or learned together with the
weights for the network. Let $\vect{h}_t^0 = E(x_t)$ denote the input vector at
time $t$ and take $\vect{h}_t^L$ as the output vector at time $t$.

We can now specify a RNN with a deterministic transition function from previous
to current states:
$$ \delta : \vect{h}_t^{l-1}, \vect{h}_{t-1}^l \rightarrow \vect{h}_t^l $$
so a basic RNN as described in Section~\ref{sec:rnn-intro} can be specified as:
$$ \delta(\vect{h}_{t-1}^l, \vect{h}_t^{l-1}) = f(\vect{W}^l_x [\vect{h}_{t-1}^l,
\vect{h}_t^{l-1}] + \vect{b}^l_x) $$
where $\vect{W}^l_x \in \mathbb{R}^{2n \times n}$, $\vect{b}^l_x \in
\mathbb{R}^n$, and $f \in \set{ \sigma, \mathrm{tanh} }$ is a nonlinearity.

Typically, in sequence prediction, the output state $\vect{h}_t^L$ at time $t$
is fed into a densely connected layer to obtain an output vector $\vect{o}_t \in
\mathbb{R}^{|\Sigma|}$:
$$ \vect{o_t} = \vect{W}_y \vect{h}_t^L + \vect{b}_y $$
where $\vect{W}_y \in \mathbb{R}^{n \times |\Sigma|}$ and $\vect{b}_y \in
\mathbb{R}^{|\Sigma|}$. 

Treating $\vect{o}_t$ as log-probabilities, we can obtain a probability
distribution over output symbols $y_t \in \Sigma$ with a softmax activation on
this layer:
$$ p(y_t) = \mathrm{softmax}(\vect{o}_t) $$
where for a vector $\vect{z} \in \mathbb{R}^n$, $\mathrm{softmax}(\vect{z})$ is
given by:
$$ \mathrm{softmax}(\vect{z})_i = \frac{ e^{z_i} }{ \sum_{j = 1}^n e^{z_j} }. $$

Figure~\ref{fig:deep-rnn-arch} illustrates the general architecture of a deep
RNN. The merging of arrows denotes \emph{vector concatenation}. We refer to the
units $A_t^l$ as \emph{cells}, specified by the transition function $\delta$.
Note that weight sharing occurs across time, but each layer has its own
parameters. This makes sense since each layer processes data of a (semantically)
different type. The initial state $\vect{h}_{\mathrm{init}}^l$ for the cells in
the $l$\textsuperscript{th} layer is typically set to $\vect{0}$ or initialised
randomly. 

\begin{figure}[H]
\centering
\includegraphics[width=350pt]{figs/deep_rnn_tmp.jpg}
\caption{Flow of information in deep RNN}
\label{fig:deep-rnn-arch}
\end{figure}

We now have the necessary machinery to introduce long short-term memory cells in
the context of deep recurrent networks. LSTM cells replace the $A_t^l$ in the
network with a complex structure centered around carefully protecting the
\emph{cell state}: an additional state vector $\vect{c}_t^l$ that allows data to
flow through many timesteps without undergoing complex non-linear information
morphing as in the case of $\vect{h}_t^l$.

Therefore, our $\delta$ function for LSTMs needs to specify the temporal
transformation of $\vect{c}_t^l$ in addition to the hidden state
transformations:
$$ \delta_{\mathrm{LSTM}} : \vect{h}_{t-1}^l, \vect{h}_t^{l-1}, \vect{c}_{t-1}^l
\rightarrow \vect{h}_t^l, \vect{c}_t^l. $$

The flow of information through an LSTM network is shown in
Figure~\ref{fig:deep-lstm-arch}.

\begin{figure}[H]
\centering
\includegraphics[width=400pt]{figs/lstm_net_tmp.jpg}
\caption{Flow of information in deep LSTM network}
\label{fig:deep-lstm-arch}
\end{figure}

Let $\ast$ denote pointwise vector multiplication. Given input
$\vect{h}_t^{l-1}$, recurrent state $\vect{h}_{t-1}^l$, cell state
$\vect{c}_{t-1}^l$, and writing $\vect{x} = [\vect{h}_{t-1}^l,
\vect{h}_t^{l-1}]$, an LSTM first computes intermediates as per equations
(\ref{eq:lstm-f}) through (\ref{eq:lstm-d}).
\begin{align}
  \vect{f} &= \sigma(\vect{W}_f \vect{x} + \vect{b}_f) \label{eq:lstm-f} \\
  \vect{i} &= \sigma(\vect{W}_i \vect{x} + \vect{b}_i) \\
  \vect{o} &= \sigma(\vect{W}_o \vect{x} + \vect{b}_o) \\
  \vect{d} &= \tanh(\vect{W}_d \vect{x} + \vect{b}_d) \label{eq:lstm-d}
\end{align}
The new states $\vect{c}_t^l$ and $\vect{h}_t^l$ are then computed with
equations (\ref{eq:lstm-c}) and (\ref{eq:lstm-h}). 
\begin{align}
  \vect{c}_t^l &= \vect{f} \ast \vect{c}_{t-1}^l + \vect{i} \ast \vect{d}
  \label{eq:lstm-c} \\
  \vect{h}_t^l &= \vect{o} \ast \tanh(\vect{c}_t^l) \label{eq:lstm-h}
\end{align}

Note that, as before, the parameters are shared across timesteps but not between
layers: layer superscripts are excluded from the parameters in the LSTM
equations for clarity. Figure~\ref{fig:lstm-cell} shows the structure of this
LSTM cell and interprets the LSTM equations graphically.

\begin{figure}[H]
\centering
\includegraphics[width=300pt]{figs/lstm_detail_tmp.png}
\caption{LSTM Cell Structure}
\label{fig:lstm-cell}
\end{figure}

The purpose of the cell state $\vect{c}_t^l$ is to act as a long-term memory.
The LSTM can be understood intuitively as protecting and using this cell state
by following three principles of selectivity:
\begin{enumerate}[label=\arabic*., itemsep=0mm]
  \item \textbf{Write} selectively: only a summary of the information entering
    the cell should be written to the protected memory.
  \item \textbf{Read} selectively: only a summary of the information stored in
    the cell's memory should be included in the output state vector.
  \item \textbf{Forget} selectively: occasionally, previously-useful memories
    will no longer be relevant, and the cell should forget them.
\end{enumerate}

The sigmoidal units $\vect{f}, \vect{i}$, and $\vect{o}$ are known as the
\emph{gates} of the LSTM cell: they are used to scale the activations of other
units componentwise and regulate the flow of information through the cell. The
function of each of the LSTM units can be understood in terms of these
principles as follows:

\begin{itemize}
  \item $\vect{f}$ is known as the \emph{forget gate}: it selectively removes
    information from the previous cell state.
  \item The tanh layer $\vect{d}$ is known as the \emph{input feature}. It
    computes data that we may wish to store in the protected cell state.
\item $\vect{i}$ is known as the \emph{input gate}: it controls which data from
  the input feature gets written into the cell state.
\item Finally, $\vect{o}$ is the \emph{output gate}: it regulates which data
  gets output based on the cell state.
\end{itemize}

Note that, since the introduction of LSTM, many other variants have appeared.
For example, an LSTM that also considers the previous cell state
$\vect{c}_{t-1}^l$ when evaluating the gate units (equations (\ref{eq:lstm-f})
through (\ref{eq:lstm-d})) is known as an LSTM with \emph{peephole} connections.
In addition, there are significant variations such as the \emph{gated recurrent
unit} (GRU). Generally, LSTM variants have only been shown to achieve marginal
performance improvements from the standard LSTM architecture (\todo cite!). We
shall only consider the architecture described thus far.

\subsection{Regularisation}

\emph{Dropout} \cite{srivastava2014dropout} is a powerful technique for
preventing overfitting in deep feedforward networks which works by randomly
shutting off some fraction of the units in some or all of the layers at train
time. This prevents excessive co-adaption among units in the network.

A recent paper by Zaremba et al.\ \cite{zaremba2014recurrent} made a major
contribution by showing how to apply dropout successfully to recurrent networks.
Although attempts had been made to apply dropout to RNNs prior to this work,
they were largely unsuccessful. In particular, applying dropout to the
\emph{recurrent} connections in a RNN leads to poor performance.

The authors found that applying dropout only to the non-recurrent connections
successfully prevents overfitting in a recurrent network and lead to improved
performance on a number of tasks. In the context of the LSTM formulation of the
previous section, we replace:
$$ \vect{x} = [\vect{h}_{t-1}^l, \vect{h}_t^{l-1}] $$
with
$$ \vect{x} = [\vect{h}_{t-1}^l, \mathrm{D}_p(\vect{h}_t^{l-1})] $$
for equations (\ref{eq:lstm-f}) through (\ref{eq:lstm-d}), where $\mathrm{D}_p$
is the \emph{dropout operator} which randomly sets each component in its result
vector to zero with probability $p$, but otherwise returns its input. 

\subsection{Tooling}

\todo Explain TensorFlow, \textbf{why it was chosen}, notion of computational
graph, etc.

\chapter{Implementation}\label{chap:impl}

\section{Corpus Preparation and Analysis}

\todo Maybe some graphs illustrating distributions in corpus as per
\texttt{analyse\_corpus.py}.

\section{Multiple Viewpoint System}

\subsection{Overview}

\begin{figure}[H]
\centering
  \trimbox{0cm 2.5cm 0cm 0cm}{ 
  \begin{tikzpicture}
  \umlclass[type=interface, template={$\zeta$,
    $T_{\text{predict}}$}]{Predictor}{}{
    +predict(context : $\zeta^*$) : dist$\langle T_{\text{predict}}
    \rangle$  \\
    +learn(sequence : $\zeta^*$)
  }
  \umlsimpleclass[y=-4, template={$\zeta$, $T_{\text{viewpoint}}$}]{GeneralViewpoint}
  \umlsimpleclass[x=9, template={$\zeta$, $T_l$, $T_r$}]{GeneralLinkedVP}

  \umlsimpleclass[x=9,y=-4, alias=seqspeclink]{SequenceModel}
  \umlsimpleclass[y=-7, alias=seqspec]{SequenceModel}
  \umlsimpleclass[y=-7,x=9, template={$T$}, alias=seqgen]{SequenceModel}
  \umlsimpleclass[y=-10,x=9, alias=ctxspec]{ContextModel}
  \umlsimpleclass[y=-10, alias=ctxgen, template={$b$}]{ContextModel}
  \umlsimpleclass[y=-13]{TrieNode}
  
  \umlimpl{GeneralViewpoint}{Predictor}
  \umlimpl{GeneralLinkedVP}{Predictor}

  \umlunicompo[arg=model, mult=1]{GeneralViewpoint}{seqspec}
  \umlunicompo[arg=model, mult=1]{GeneralLinkedVP}{seqspeclink}
  \umlreal[stereo={$T \rightarrow T_{\text{viewpoint}}$}]{seqspec}{seqgen}
  \umlreal[stereo={$T \rightarrow \text{Pair}\langle T_l, T_r
  \rangle$}]{seqspeclink}{seqgen}
  
  \umlunicompo[arg=model, mult=1]{seqgen}{ctxspec}
  \umlreal[stereo={$b \rightarrow T::\text{cardinality}$}]{ctxspec}{ctxgen}

  \umlunicompo[arg=trie root, mult=1]{ctxgen}{TrieNode}
  \umlunicompo[arg=children, mult=0..b, recursive=-90|2|3cm]{TrieNode}{TrieNode}
\end{tikzpicture}
}
\hspace{-10mm}
\caption{UML Class Diagram of Prediction Stack}
\label{fig:uml-prediction}
\end{figure}

A multiple viewpoint system was implemented in \texttt{C++} in the form of a
generic framework for multiple viewpoints and specific code to represent
chorales and viewpoints for the chorale dataset. A design goal of the
implementation was to achieve a symmetry and tight correspondence between the
\texttt{C++} type system and the formalism of multiple viewpoints detailed in
Section~\ref{sec:mvs-formalism}.

The implementation achieves much generality by extensive use of \texttt{C++}
\emph{templates}. This is illustrated in Figure~\ref{fig:uml-prediction}: a UML
class diagram outlining the ``prediction stack'': the generic portion of the
viewpoint implementation. As we shall see, specific viewpoints can be
implemented by specialising the template classes \texttt{GeneralViewpoint} and
\texttt{GeneralLinkedVP}. 

\begin{figure}[H]
\centering
  \trimbox{0cm 0.0cm 0cm 0cm}{ 
  \begin{tikzpicture}
  
  \umlclass{ChoraleMVS}{
    - short\_term\_layer : ChoraleVPLayer \\
    - long\_term\_layer : ChoraleVPLayer
  }{
    + add\_viewpoint$\langle T \rangle$(vp : ChoralePredictor$\langle T\rangle$ *) \\
    + predict$\langle T\rangle$(context : vec$\langle$ChoraleEvent$\rangle$) :
    dist$\langle T \rangle$ \\
    + cross\_entropy(seq : vec$\langle$ChoraleEvent$\rangle$) : double \\
    + generate(length : int, ts : TimeSig) : vec$\langle$ChoraleEvent$\rangle$
    \\
    + learn(example : vec$\langle$ChoraleEvent$\rangle$)
  }
  \umlsimpleclass[y=-5]{ChoraleVPLayer}
  \umlsimpleclass[x=-5, y = -9, alias=pitchpred]{Predictor}
  \umlsimpleclass[x=0, y = -9, alias=durpred]{Predictor}
  \umlsimpleclass[x=5, y = -9, alias=restpred]{Predictor}
  \umlsimpleclass[y = -13, type=interface, template={$T$}, alias=cpred]{Predictor}
  \umlsimpleclass[y = -17, type=interface, template={$\zeta$, $T$},
alias=predbase]{Predictor}
  
  \umlunicompo[arg=short-term, mult=1, anchors=-115 and 160]{ChoraleMVS}{ChoraleVPLayer} 
  \umlunicompo[arg=long-term, mult=1, anchors=-65 and 20]{ChoraleMVS}{ChoraleVPLayer}
  \umluniaggreg[arg=pitch\_vps, mult=1..*]{ChoraleVPLayer}{pitchpred}
  \umluniaggreg[arg=duration\_vps, mult=1..*]{ChoraleVPLayer}{durpred}
  \umluniaggreg[arg=rest\_vps, mult=1..*]{ChoraleVPLayer}{restpred}
  \umlreal[stereo={$T \rightarrow$ ChoralePitch}, pos stereo=0.3]{pitchpred}{cpred}
  \umlreal[stereo={$T \rightarrow$ ChoraleDuration}, pos stereo=0.55]{durpred}{cpred}
  \umlreal[stereo={$T \rightarrow$ ChoraleRest}, pos stereo=0.3]{restpred}{cpred}
  \umlreal[stereo={$\zeta \rightarrow$ ChoraleEvent}]{cpred}{predbase}
\end{tikzpicture}
}
\hspace{-10mm}
\caption{UML Class Diagram relating viewpoint framework with Chorale
hierarchy}
\label{fig:chorale-uml}
\end{figure}

Figure~\ref{fig:chorale-uml} shows how the general viewpoint framework is
specialised to implement the components specific to our chosen corpus. In
particular, it outlines \texttt{ChoraleMVS}: a high-level class which manages
all the components of a multiple viewpoint system over chorales and provides a
unified interface for tasks such as \emph{prediction} and \emph{generation}.

We shall now give a brief overview of the components mentioned so far and their
function to gain an appreciation of the intention behind each component and how
the overall system fits together before taking a closer look at various parts of
the implementation.

\begin{itemize}
  \item \texttt{TrieNode<b>} is a trie implementation with branching factor
    \texttt{b}.
  \item Using this trie implementation, \texttt{ContextModel<b>} implements a
    context model over an alphabet of size \texttt{b}, as per
    Section~\ref{sec:ctx-model-prep}. The inference algorithm is PPM A with
    exclusion.
  \item A simple, unified representation for domain-specific types is used
    throughout.  These are passed as template parameters to many different
    classes. Examples include basic types such as \texttt{ChoralePitch} and
    \texttt{ChoraleDuration}, derived types such as \texttt{ChoraleInterval} and
    abstract meta-types such as \texttt{EventPair<T1,T2>}.
    
  \item \texttt{SequenceModel<T>} further abstracts \texttt{ContextModel} by
    implementing a higher-level interface to context models in terms of these
    domain-specific types.
  \item \texttt{EventDistribution<T>} represents a probability distribution over
    a type \texttt{T}. It supports multiple methods of distribution combination
    as per Section~\ref{sec:vp-comb}.
\end{itemize}

\todo Then we can really dive into the chronological-ish (primarily, bottom-up)
section-by-section approach that follows.

\subsection{Context Models}

\subsection{Event Representation}

\subsection{Sequence Models}

\subsection{Distribution Combination}

\subsection{Early Viewpoints}

\subsection{Linked Viewpoints}

\subsection{Generalised Viewpoints}

\section{Recurrent Neural Network}

\todo Obviously work out where this should go, but remember that the TF LSTM
implementation adds a \texttt{forget\_bias} of 1 to the biases of the forget
gate because it helps towards the beginning of training. This should be the only
difference with the LSTM architecture presented in the preparation.

\subsection{Early Experiments}

\todo Mention language modelling precursor: maybe include snippet of output?

\subsection{Corpus Preprocessing}

\todo Describe the method of \emph{tonal inflation} in order for the RNN to
learn the key as an emergent property of each composition.

\subsection{Domain-specific Features}

\todo Explain ``clock'' inputs. Discuss symmetry with multiple viewpoints.

\section{Testing and Debugging}

\todo Get lots of points and stickers for demonstrating good software
engineering practice by testing thoroughly.

\chapter{Evaluation}\label{chap:eval}


\chapter{Conclusion}

\todo Write me.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\phantomsection
\printbibliography
\addcontentsline{toc}{chapter}{Bibliography}
\todo Check all the references for typos as Google Scholar certainly has a few!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

\todo uncomment me!
%\input{proposal}

\end{document}
