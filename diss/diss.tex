% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[UKenglish]{isodate}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{parskip}
\usepackage{verbatim}
\usepackage{color}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lilyglyphs}
\usepackage{fontspec}
\usepackage{mathtools}
\usepackage{float} % figure positioning
\usepackage{bm} % bold symbols in mathmode
\usepackage{minted} % syntax highlighting
\usepackage{csquotes}

% tikz for drawing
\usepackage{tikz}
\usepackage{tikz-uml}

% pseudocode typesetting
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

% math macros
\newcommand{\set}[1]{ \left\{ #1 \right\} }
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

% music notation
\newcommand{\insharp}[0]{\sharp[raise=0.1,scale=0.8]}
\newcommand{\inflat}[0]{\flat[raise=0.1,scale=0.8]}

\usepackage[style=numeric,backend=biber]{biblatex}
\addbibresource{refs.bib}

\usepackage{docmute}   % only needed to allow inclusion of proposal.tex

\newcommand{\todo}{\textcolor{red}{\textbf{todo}~}}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

% The `minted` package for sytnax highlighting introduces a `listing`
% environment (and associated counter) which can be used to wrap code snippets
% in a float with a caption.
%
% However, these are not set up to use chapter-numbering, so we set this up
% here.
\usepackage{chngcntr}
\counterwithin{listing}{chapter}

% quick macro for inline C++
\newcommand{\cppi}[1]{{\small \mintinline{cpp}{#1}}}

\begin{document}

\cleanlookdateon

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title

\pagestyle{empty}

\rightline{\LARGE \textbf{Alex Coplan}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{A Comparison of Statistical Models and Recurrent Neural Networks for the
Generation of Music} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
St Catharine's College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{r p{10.5cm}}
Name:               & \bf Alex Coplan                       \\
College:            & \bf St Catharine's College                     \\
Project Title:      & \bf A Comparison of Statistical Models and Recurrent
Neural Networks for the \newline Generation of Music \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2017  \\
Word Count:         & \todo\footnote{1} \\
Project Originator: & Alex Coplan \\
Supervisor:         & Matthew Ireland                    \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

The original aim of this project was to implement two models for music
generation and subsequently compare them: namely, a \emph{recurrent neural
network} and \emph{multiple viewpoint system}. The two models were
to be compared using both a listening survey involving human participants and
objective metrics of evaluation, such as information-theoretic measures
of predictive performance.

\section*{Work Completed}

All that has been completed appears in this dissertation.

\section*{Special Difficulties}

\todo
 
\newpage
\section*{Declaration}

I, Alex Coplan of St Catharine's College, being a candidate for Part II of the
Computer Science Tripos, hereby declare that this dissertation and the work
described in it are my own work, unaided except as may be specified below, and
that the dissertation does not contain material that has already been used to
any substantial extent for a comparable purpose.

\bigskip
\leftline{Signed }

\medskip
\leftline{Date }

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

Acknowledge acknowledge acknowledge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}

The modelling and automated generation of music is a central task in an approach
to understanding computational creativity. It is natural to ask whether
computers can produce music that is compelling to humans, and indeed, this
question has long been posed by researchers, with practical efforts dating back
to the mid-1950s \cite{ames1987automated}. 

The aim of this work is to implement and compare two modern techniques for
\emph{melody generation}: namely, \emph{recurrent neural networks} and
\emph{multiple viewpoint systems}.

An automated system for melody generation is motivated by end-user applications
in \emph{computer-assisted composition}, whereby the system can provide
inspiration for a human composer, either by generating entirely novel melodies
within stylistic constraints, or extemporising on or extending melodic
fragments written by the human composer. Such a system might augment the
capabilities of typical music notation software. 

Markov modelling is a simple yet effective technique for capturing the
statistics of sequential data. An obvious tool to apply to the modelling of
melody is the Markov chain \cite{ames1989markov}. However, an important
observation to make is that music has a rich underlying structure: modelling the
statistics of notes directly (the \emph{surface structure}) is insufficient to
capture the complex language of musical style. This observation motivated the
development of more sophisticated models, known as \emph{multiple viewpoint
systems} \cite{conklin1995viewpoints}.  A multiple viewpoint system exploits the
rich underlying event structure of complex languages by combining the
predictions of an ensemble of context models, each modelling a different
attribute of the event space.

Recurrent neural networks (RNNs), the natural topology of neural network for
modelling sequential data, have been widely applied to tasks such as language
modelling \cite{graves2013generating}, machine translation
\cite{sutskever2014sequence}, and indeed to the modelling of music
\cite{boulanger2012modeling}. A RNN is an end-to-end sequence learning tool
which relies on very little domain knowledge aside from the chosen input
representation. A consequence of this domain-independence is that a system
designed for character-level language modelling can equally be trained to model
melody without changing the network architecture. In practice, however, we shall
see that slight architectural modifications can be beneficial.

In this work, we compare the performance of multiple viewpoint systems with that
of recurrent neural networks on a task of stylistically-constrained melody
generation. In particular, we assess predictive performance, in terms of a
\emph{cross-entropy} loss function, on a corpus of melodies used in the chorale
harmonisations of J.S.\ Bach. Furthermore, we compare the sampled outputs from
each model by means of a listening survey involving human participants.

A high-level goal of this work which motivates the choice of models involved is
understanding to what extent \emph{feature engineering} impacts the
effectiveness of musical models. A recurrent neural network, at one extreme, is
a domain-independent, end-to-end learning system, capable of learning to extract
high-level features from sequential data \cite{Goodfellow-et-al-2016}. A system
of viewpoints, however, relies on its creator to use domain knowledge to
determine a set of salient features, encoded in the form of the \emph{pool} of
viewpoints made available to the system.

\section{Background}

\subsection{Music Theory}

The development and implementation of the recurrent neural network in this work
can be understood with minimal knowledge of music theory. In general, the
multiple viewpoint formalism can also be expressed independently of any domain
knowledge.  However, to understand the application of the multiple viewpoint
framework to music requires an elementary understanding of music theory.  This
section introduces the relevant terminology to enable such discussion.

\begin{figure}[H]
\centering
\includegraphics[width=350pt]{figs/piano_spn_tmp.jpg}
\caption{Keyboard illustrating pitches in chromatic scale annotated using SPN}
\label{fig:keyboard-spn}
\end{figure}

We shall constrain our discussion to the context of Western classical music,
since the style which we wish to model lies within this context. A \emph{pitch}
is an abstract concept which is related to the frequency of a musical note.
Formally, if $p,q$ are pitches with corresponding frequencies $\nu_p,\nu_q$,
then if $\nu_q = 2\nu_p$, $p$ and $q$ are said to span an \emph{octave}, with
$q$ an octave above $p$. In Western classical music, each octave is divided into
twelve distinct \emph{note names} (A through G modified by accidentals
\insharp{} or \inflat{}). A note name, such as `B\inflat' (or, equivalently,
A\insharp), is typically thought of as not just a single pitch, but a
\emph{pitch class}, the set of all pitches an integer number of octaves above or
below any pitch with that note name.  Formally, given some reference frequency
$\nu_n$ for a note name $n$, the set of frequencies of the pitches in the pitch
class for $n$ is given by $\set{ 2^k \cdot \nu_n\ |\ k \in \mathbb{Z} }.$

The mapping between pitch and frequency is known as \emph{temperament}, which on
modern instruments is typically \emph{equal}, meaning that the frequency space is
equally divided among the twelve pitches in an octave. More precisely, the
$(k+1)$\textsuperscript{th} note of the chromatic scale with base frequency $\nu_0$ is
given by $2^{k/12}\cdot\nu_0$. As an aside, all recordings of
model outputs in this work were made using equal temperament. Herein
we will work with the abstraction of pitch, ignoring the underlying frequencies
of notes.

To refer to specific pitches, we make use of \emph{scientific pitch notation}
(SPN). Write $N_k$ where $N$ is a note name and $k$ is an octave number. To
ground all the definitions made thus far, we use the standard reference
frequency $\mathrm{A}_4 \triangleq 440\ \mathrm{Hz}$.
Figure~\ref{fig:keyboard-spn} shows the SPN labelling of a chromatic scale. An
\emph{interval} refers to the difference in pitch between two notes. Consecutive
note names are said to be a \emph{semi-tone} apart, and an interval of two
semi-tones is known as a \emph{tone}. Later in this work we shall further
abstract pitch and intervals using MIDI pitch numbering, and further concepts
shall be defined as necessary. In the MIDI tuning standard, $\mathrm{A}_4$
corresponds to pitch number $69$.

Figure~\ref{fig:note-values} shows the musical division of time. Along the left
are the British English names we shall use to refer to note values. The speed at
which a piece is played can be specified by fixing the number of times any of these
note values (most commonly, the crotchet) occur in one minute.

\begin{figure}[H]
\centering
\includegraphics[width=400pt]{figs/note_values_tmp.jpg}
\caption{Notation for Duraitons of (a) rests and (b) notes}
\label{fig:note-values}
\end{figure}

A dot following a note or rest such as \crotchetDotted{} or \quaverRestDotted{}
indicates that the duraiton is $1.5$ times the length of its non-dotted
counterpart.

Moreover, Western classical music is typically divided into regular units of
musical time known as \emph{bars}. Both the length of each bar and the regular
rhythmic stress is specified by the \emph{time signature}. The only two time
signatures of concern in this work are \lilyTimeSignature{3}{4} and
\lilyTimeSignature{4}{4}. The numerator indicates the number of \emph{beats} in
a bar, and the demoninator the \emph{duration} of each beat. It is important to
be aware that many musical effects are related to the position of a note in the
bar. For example, in \lilyTimeSignature{4}{4}, beats $1$ and $3$ are referred to
as the \emph{strong} beats of the bar and are associated with certain rhythmic
tendencies. We shall exploit this property in the implementation of both
techniques.

\todo tonality???

\subsection{Problem Domain}

In Conklin's 1990 thesis \cite{conklin1990prediction}, he discusses some of the
inherent difficulties in the task of melody generation:

\begin{displayquote}
``Learning plausible continuations in a musical style presents difficulties not
encountered in other domains. In addition to capturing global stylistic rules, a
system must also capture sequential structure, pattern, and repetition within an
individual work.''
\end{displayquote}

Since there are many plausible continuations for a given melodic fragment, this
suggests that probabilistic machine learning techniques will be a useful
approach to take, and casts doubt on the effectiveness of knowledge-based
techniques which attempt to compute exact solutions using hard-coded inference
rules.

\section{Related Work}

\todo Mention GAs.

\subsection{Basic Markov models}

Markov modelling is a technique that has long been applied to music. For a
review, see Ames \cite{ames1989markov}. Basic $n$-gram approaches such as
first-order, higher-order, or indeed variable-order Markov chains over primitive
musical events are all fundamentally limited in that they require an exact match
of a musical context (of some length) in order to make useful predictions. This
is an issue that is addressed by the \emph{multiple viewpoint} approach to
modelling music.

\subsection{Knowledge-based Systems}

Prior to the statistical viewpoint systems investigated in this work, ideas
relating to the method of multiple viewpoints were first applied to music in
1986 by Ebcioğlu in a rule-based system for chorale harmonisation
\cite{ebcioglu1986expert}.  This system used hand-crafted rules written in
first-order logic which were expressed in terms of different viewpoints of the
music. 

\subsection{Multiple Viewpoints}

In 1988, Conklin and Cleary \cite{conklin1988modelling} applied multiple
viewpoints with underlying probabilistic Markov models to modelling Gregorian
chant and simple two-part polyphony. The authors make use of the
\emph{prediction by partial match} (PPM) algorithm \cite{cleary1984ppm} for
smoothing variable-order Markov models, but the escape method used is not
specified.  An ad-hoc, unweighted method is used for combining the predictions
of viewpoints.

Conklin and Witten went on to develop a formalism around multiple viewpoints in
a key 1995 paper \cite{conklin1995viewpoints}. The authors justify discounting
the
\emph{knowledge engineering} approach:

\begin{displayquote}
``There are too many exceptions to any
logical system of musical description, and it will be difficult to ensure the
completeness of an intuited theory. The system will always exclude some valid
pieces. The generations of a theory are bound to reflect the biases of an
engineer; this is the only way they might be called creative.''
\end{displayquote}

With similar reasoning, we shall only consider models which learn from data.  At
this point in time, the convention in the literature became to use ``multiple
viewpoint system'' to refer to \emph{statistical} viewpoint systems with
underlying context models: a convention we shall also adopt.

Many of the ideas in this paper were first published in Conklin's 1990 thesis
\cite{conklin1990prediction}. The notion of separate \emph{long-term} and
\emph{short-term} models in a viewpoint system was introduced in this work.
Conklin also details a principled method for distribution combination, namely
\emph{entropy-weighted arithmetic combination}. 

Among the contributions from his 2005 thesis \cite{pearce2005construction},
Pearce introduces a \emph{geometric} viewpoint combination technique, shown to
outperform its arithmetic counterpart, as well as a method for automatic
viewpoint selection, thereby greatly reducing the selection bias in multiple
viewpoint systems. The work concerns the modelling of melody with a central
focus on \emph{pitch}, as this is deemed to be the most complex of the musical
dimensions.

Whorley's thesis of 2013 \cite{whorley2013phd} deals with the application of
multiple viewpoints to modelling four-part harmony. In addition to improved
algorithms for viewpoint selection, Whorley makes use of a much larger pool of
viewpoints than had previously been available, in particular due to the demands
of modelling four-part harmony.

\subsection{Early Connectionist Approaches}

\begin{figure}[H]
\centering
\includegraphics[width=110pt]{figs/windowed_nn_tmp.jpg}
\caption{Windowed feedforward neural network}
\label{fig:windowed-nn}
\end{figure}

Early connectionist approaches to modelling sequential data adapt conventional
feedforward networks to the task by using a finite \emph{windowed} context of
size $N$ and training the network to predict the $N$ events that follow
\cite{todd1989connectionist}. Figure~\ref{fig:windowed-nn} illustrates this
approach.

Observe that, in such an architecture, any common effects between e.g.\ $e_1$
and $e_2$ must be learnt entirely separately from the effects between $e_2$ and
$e_3$, even though there may be a considerable amount of time-invariant
regularity between consecutive events in the sequences of interest. Similarly,
when predicting the output $e_{N+1}^{2N}$, the network will have to learn how to
predict each of these events independently from each other (that is, without
time invariance).  

It can be seen therefore that such networks are very \emph{inefficient} sequence
learners: with a large $N$, vast amounts of training data will be needed and
many parameters to successfully capture any time-invariant effects. Moreover,
with a small $N$ the network will fail to capture any long-term effects.

One solution to this problem is \emph{parameter sharing}, and is central to the
sequence-learning ability of \emph{recurrent neural networks} (RNNs). To quote
the recent text of Goodfellow et al.\ \cite{Goodfellow-et-al-2016}:
\begin{displayquote}
  ``If we had separate parameters for each value of the time index, we could not
  generalise to sequence lengths not seen during training, nor share statistical
  strength across different sequence lengths and across diﬀerent positions in
  time.''
\end{displayquote}

\subsection{Recurrent Neural Networks}

Recurrent Neural Networks (RNNs), unlike feedforward networks, maintain a
\emph{hidden state} and learn to evolve this hidden state using parameters
specifying transitions which are shared over time. Rumelhart and Hinton et al.\
first showed how to train such networks with backpropagation in 1985
\cite{rumelhart1985learning}. RNNs are clearly a natural tool to apply to
sequnece learning. However, basic RNNs are known to suffer from the problem of
\emph{vanishing and exploding gradients}. This problem severely hinders the
ability of basic RNNs to learn long-term dependencies: a capacity which is
crucial in our problem domain.

In 1997, the introduction of the \emph{long short-term memory} (LSTM)
architecture by Hochreiter and Schmidhuber \cite{hochreiter1997long} enabled the
construction of RNNs capable of learning dependencies over many more timesteps
than was previously possible. Today, \emph{gated} architectures such as the LSTM
achieve state-of-the-art results in sequence prediction \cite{zaremba2014recurrent}.

\section{Context of the Work}\label{sec:context-of-work}

In the connectionist literature on modelling music, $n$-gram models are often
used as a baseline approach (e.g.\ \cite{boulanger2012modeling}
\cite{liang2016bachbot}). However, such $n$-gram models are typically only
models of the surface structure, and sophisticated models such as multiple
viewpoint systems are not usually considered.

Similarly, in the literature on multiple viewpoints, performance comparisons to
modern connectionist techniques are rarely drawn. In particular, to the best of
my knowledge, a direct comparison between multiple viewpoint systems and
recurrent networks does not exist. This work therefore draws an interesting, 
novel comparison between two sophisticated yet contrasting approaches.

\section{Structure}

\todo

\chapter{Preparation}

\section{Choice of Models}

Despite the strong performance of multiple viewpoint systems when applied to
music, as discussed in Section~\ref{sec:context-of-work}, multiple viewpoint
systems are rarely used as a baseline approach or brought into comparison with
mainstream machine learning techniques such as neural networks. The reasons for
this, we argue, are three-fold:
\begin{itemize}
  \item Considerable domain-specific knowledge is required to implement multiple
    viewpoint systems.
  \item The implementation of a multiple viewpoint system is reasonably involved
    for a baseline approach.
  \item To the best of my knowledge, no open source framework for implementing
    multiple viewpoint systems exists.
\end{itemize}

Recurrent Neural Networks, and LSTM networks in particular, are the most
successful general sequence-learning models in use at this time. Primarily,
therefore, it is the lack of comparison between these two models that motivates
this choice. Secondarily, as a by-product of choosing to implement a general
framework for multiple viewpoints, it is my intention that an open source
project that may be of use to researchers for further work can be released.

\section{Starting Point}

Prior to starting this work, I had given a talk on basic $n$-gram approaches to
melody generation using first-order Markov models of melody. This was
accompanied by some code for demonstration puprposes. Other than this, I have
not done previous research or implementation in this area. All of the code
written for this project, aside from established libraries, was written from
scratch.

\section{Deliverables}

The success criteria for the project comprise the following deliverables:
\begin{itemize}
  \item A program or tool for importing the corpus in the desired format.
  \item A multiple viewpoint system (MVS) capable of generating melody.
  \item A recurrent neural network (RNN) capable of generating melody.
  \item Quantitative and human evaluation.
\end{itemize}

This chapter concerns the ideas behind the first three of these, along with
a discussion of the research and decisions made prior to implementation.
Chapter~\ref{chap:eval} will discuss the design and implementation of the
human evaluation survey in detail, as well as quantitative evaluation.

\section{Choice of Corpus and Representation}\label{sec:corp-rep}

The chosen corpus was a set of chorale melodies used in the harmonisations of
J.S.\ Bach. The Bach Chorales are a corpus of great interest in the
computational modelling of music: the style is unified, yet with significant
inter-opus variance; it is not too complex, yet not overly simplistic.

An internal format for melody based on a representation used commonly in the
multiple viewpoint system literature (e.g.\ \cite{conklin1995viewpoints}) was
decided on. It was decided that musical time should be quantised with a quantum
of $1/16$\textsuperscript{th} notes (semiquavers). 

This representation was to be used internally for both the MVS and
RNN implementations. A note $n$ is represented as a tuple $(p,o,d) \in
\mathbb{N}^3$ where $p$ is the MIDI pitch number of $n$, $o$ is the onset 
time, and $d$ is the duration. A melody is then a list of such tuples. Note that
rests are not explicitly represented as events in this encoding.

\section{Overview of Music Generation}\label{sec:gen-models}

A system for music generation need not be probabilistic. Techniques such as
genetic algorithms and rule-based systems are capable of generating output
without modelling a probability distribution over the output space. Following
the reasoning of the previous chapter, however, we shall exclusively consider
probabilistic models. 

Any system which models (either implicitly or explicitly)
a probability distribution over sequences of interest is said to be a
\emph{generative model}. Let $\vect{x} = x_1, \ldots, x_t$ range over sequences
of interest. Then a model which calculates or approximates $p(\vect{x})$ is said
to be a generative model.

This distribution is commonly approximated by assuming that each event only
depends on those that come before it, yielding (\ref{eq:cond-indep}).  All the
models we work with will make this assumption of conditional independnece.
\begin{equation}
  p(\vect{x}) = p(x_t | x_{t-1}, \ldots, x_1) p(x_{t-1} | x_{t-2},
  \ldots, x_1) \cdots p(x_1) \label{eq:cond-indep}
\end{equation} 

An even stronger assumption is the \emph{Markov assumption}, which assumes that
we only need to look at a finite context of length $n$ to be able to predict the
future, i.e.
$$ p(x_k | x_{k-1}, \ldots, x_1) \approx p(x_k | x_{k-1}, \ldots, x_{k-n}). $$

In any case, given some generative model approximating $p(\vect{x})$, we can
\emph{sample} from this distribution to obtain an output from the model. In the
case of models that make the assumption of (\ref{eq:cond-indep}), we can use
sampling by \emph{random walk}. At time $t$, for $t = 1$ to $T$, we sample from:
$$ p(x_t | x_{t-1} \cdots x_1) $$
and immediately accept the resulting event $x_t$, appending it to our sample.

While random walk sampling is very efficient, running in $\Theta(T)$ time to
generate a sample of length $T$, it can sometimes generate solutions $\vect{x}'$
with low $p(\vect{x}')$. Typically, we want to draw samples that our model
assigns high probabilities. To mitigate this, one can simply repeatedly draw
samples $\vect{x}'$ by random walk until $p(\vect{x}') > \alpha$ for some
threshold $\alpha$: this algorithm is known as \emph{iterative random walk}.

Both of the models we shall consider are generative models that approximate
$p(\vect{x})$ using the assumption of (\ref{eq:cond-indep}). To generate
candidate $\vect{x}'$ from these models, we sample from their distributions by
means of (iterative) random walk.

\section{Multiple Viewpoint Systems}

In this section, the key theory, algorithms, and data structures underpinning
multiple viewpoint systems are introduced. The notation and formalism is as per
Conklin and Witten \cite{conklin1995viewpoints}. We start by defining some
notation which will be used throughout this section.

\begin{itemize}[itemsep=0mm]
  \item If $\tau$ is a type, then $[\tau]$ is the \emph{syntactic domain} of
    that type: all elements of type $\tau$.   
  \item $S^*$ denotes the set of all sequences drawn from a set $S$.
  \item $e_i^j$ abbreviates the sequence $(e_i,e_{i+1},\ldots,e_{j-1},e_j)$ and
    $()$ the empty sequence.
  \item $s :: e$ denotes the sequence $s$ with event $e$ appended.
  \item $\zeta$ denotes the \emph{event space}, the set of all possible events
    that can occur in sequences of interest. For example, a very simple event
    space for melody might be:
    $$ [\mathrm{pitch}] \times [\mathrm{duration}] \times [\mathrm{onset}]. $$
\end{itemize}

\subsection{Context Models}\label{sec:ctx-model-prep}

The central primitive underlying a multiple viewpoint system is the
\emph{context model}. In general, a context model over some type $\tau$ is a
data structure storing sequences in $[\tau]^*$ together with an inference
procedure predicting distributions over $[\tau]$ given some conext in
$[\tau]^*$. The inference procedure in such a model relies on the Markov
assumption, as detailed in Section~\ref{sec:gen-models}.

While first-order Markov chains make predictions based on just one timestep of
context, an $n$\textsuperscript{th} order Markov chain makes predictions based
on $n$ elements of context. For notational convenience, and for consistency with
the literature, we generally refer to the \emph{history} $h = n+1$ of an
$n$\textsuperscript{th} order Markov model. Such a model stores and models
$h$-grams.

The context models I implement are variable-order Markov models with some
maximum history $\hbar$. Such a model can be thought of as a combination of
Markov models with history $1,2,\ldots,\hbar$. 

A known problem with high-order Markov models is their space complexity. A naïve
tabular approach uses $\Theta(|[\tau]|^h)$ space.  In practice, such high-order
models are \emph{sparse}: most $h$-grams are never seen in the training data. A
data structure which exploits this sparsity is the \emph{suffix tree} or
\emph{trie}.  In such a structure, the nodes are events, each with an associated
count, and a path from the root corresponds uniquely to a particular $h$-gram.
Figure~\ref{fig:dur-trie} illustrates this data structure, in this case the trie
supporting a context model over musical durations. 

\begin{figure}[H]
\centering
\includegraphics[width=455pt]{figs/duration_vp.pdf}
\caption{Trie for a context model over duration with $\hbar = 3$}
\label{fig:dur-trie}
\end{figure}

The algorithm for extracting $h$-grams for a training sequence $e_1^k$ works by
passing a sliding window of size $\hbar$ across the input sequence and
generating $h$-grams in this window of size $1$ through $k$. 

\subsubsection{Inference from Context Models}

Once we have constructed the trie for a context model, we need an algorithm to
perform \emph{inference}. In particular, we want to compute $\mathbb{P}(e' |
e_1^k)$ for some next event $e' \in [\tau]$ and input context $e_1^k \in
[\tau]^*$.

The maximum likelihood solution to this problem is simply to use the relative
frequency of the $h$-gram of interest:
\begin{equation}
  \mathbb{P}(e'|e_1^k) = \frac{ C(e_1^k::e') }{ \sum_{e''} C(e_1^k::e'') }
  \label{eq:ctx-max-like}
\end{equation}
where $C(e_i^j)$ denotes the count associated with $e_i^j$ in the trie, or $0$
if such a node does not exist. Note that all sequences longer than $\hbar$ (and
contexts longer than $\hbar-1$) are implicitly ignored, so $\forall j \in
\mathbb{N}.\ C(e_{k-\hbar-j}^k) = C(e_{k-\hbar+1}^k)$ and
$\mathbb{P}(e'|e_{k-\hbar+1-j}^k) = \mathbb{P}(e'|e_{k-\hbar+2}^k)$.

There are two main problems with this solution. The first is that we want our
models to be \emph{non-exclusive}; that is, $\forall e' \in [\tau].\
\mathbb{P}(e' | e_1^k) > 0$. An \emph{exclusive} model would limit the scope for
creativity in the system, and moreover, from a practical standpoint, evaluation
metrics such as \emph{cross-entropy} require calculating log-probabilities: we
therefore want to avoid zero probabilities. This model, however, fails to
guarantee non-exclusivity.

The second problem with this solution is that it does not deal with \emph{novel
contexts}. In particular, suppose we have not seen the context $e_1^k$. Then,
clearly, the denominator of (\ref{eq:ctx-max-like}) will be zero, which is
equally problematic.

An algorithm which attends to both of these issues is \emph{prediction by
partial match} (PPM) \cite{cleary1984ppm}. The central idea behind PPM is that,
instead of distributing the probability mass entirely among the exact context
matches, as per (\ref{eq:ctx-max-like}), we instead reserve some amount of the
probability mass known as the \emph{escape probability}. The escape probability
is then distributed recursively among a lower-order model. In this sense, PPM
is a form of \emph{backoff smoothing}.

Backoff smoothing algorithms are typically formulated recursively, as per
(\ref{eq:ppm-general}).
\begin{equation}\label{eq:ppm-general}
  \mathbb{P}(e' | e_1^k) = \begin{cases}
  \alpha(e'|e_1^k) & C(e' | e_1^k) > 0 \\
\gamma(e_1^k) \cdot \mathbb{P}(e' | e_{(k - \hbar) + 2}^k) & \text{otherwise}
\end{cases} 
\end{equation} 

If no match is found for any model, the recursion bottoms out with a uniform
distribution. The choice of \emph{escape probability} $\gamma(\cdot)$ is known
as the \emph{escape method}: established methods include A, B, C, D, and AX
\cite{pearce2004improved}. PPM A, the method implemented in this work, uses the
following $\alpha$ and $\gamma$ functions:
\begin{align}
  \label{eq:ppm-a}
  \alpha(e' | e_1^k) &= \frac{ C(e' | e_1^k) }{ 1 + \sum_{e''} C(e'' | e_1^k) }
  \\
  \gamma(e_1^k) &= \frac{ 1 }{ 1 + \sum_{e'} C(e' | e_1^k) }
  \label{eq:ppm-escape}
\end{align}

This recursive formulation can be somewhat misleading, as the notation implies
that (\ref{eq:ppm-general}) gives rise to properly normalised probability
distributions.  This is in fact not the case. Consider performing PPM with
$\hbar = 2$ over an alphabet $\Sigma = \set{\alpha,\beta,\gamma}$, and suppose
we learn the sequence $(\alpha,\beta,\gamma,\alpha,\alpha,\beta)$.
Figure~\ref{fig:bad-ppm-trie} illustrates the situation, and includes virtual
$\epsilon$-nodes to represent the escape mass.

\begin{figure}[H]
\centering
\includegraphics[width=400pt]{figs/problematic_trie_tmp.jpg}
\caption{Trie demonstrating lack of normalisation in standard PPM.}
\label{fig:bad-ppm-trie}
\end{figure}

The first problem is the escape node at layer $h = 1$. According to
(\ref{eq:ppm-escape}) it should have an effective count of $1$. However, this
yields:
$$ \sum_{e \in \Sigma} \mathbb{P}(e) = \frac{3}{7} + \frac{2}{7} + \frac{1}{7} =
\frac{6}{7} \neq 1. $$
The observation to make here is that if \emph{all} symbols in our alphabet have
a non-zero count for any given context, then the escape probability is
redundant. Suppose, then, we set the effective count of this $\epsilon$-node to
zero. Consider the distribution $\mathbb{P}(e|\alpha)$. We can trivially compute
$\mathbb{P}(\alpha|\alpha) = 1/4$ and $\mathbb{P}(\beta|\alpha) = 1/2$, and for
$\gamma$ we escape to find $\mathbb{P}(\gamma|\alpha) = 1/4 \cdot
\mathbb{P}(\gamma)$. However, $\mathbb{P}(\gamma) < 1$, so
$\mathbb{P}(e|\alpha)$ is not normalised. The problem here is that the escape
mass for the node labelled $\alpha\epsilon$ is distributed among all the symbols
at layer $h = 1$ in the trie, when in reality we can only ever escape to
$\gamma$.

There are two typical solutions for this lack of normalisation in PPM: in
earlier work, authors often simply compute an improper distribution and later
normalise it (e.g.\ \cite{conklin1990prediction}). A more sophisticated and less
wasteful solution which reasons about the possible $h$-grams that can be escaped
to is known as PPM with \emph{exclusion} \cite{pearce2004improved}. I implement
exclusion for the context models in this work.

\subsection{Viewpoints}\label{sec:mvs-formalism}

One possible approach to modelling music would be to form a context model over
the entire event space $\zeta$.  Recall that $\zeta = [\tau_1] \times [\tau_2]
\times \cdots \times [\tau_n]$ is the cartesian product of many types. Aside
from being computationally expensive, this approach is severely limited in its
predictive power, since it requires an \emph{exact match} of the musical
context to make useful predictions: that is, there is no ability of the system
to \emph{generalise} to unseen contexts.

An obvious approach to allow the system to generalise would be to model some or
all of the $\tau_i$ \emph{independently}. To see why this might be useful,
consider that while there is certainly some correlation between e.g.\ pitch and
duration, much of the rhythmic regularity in a particular corpus is independent
of the pitch of the notes: thus, it is likely to be fruitful to model duration
independently of any of the other musical attributes.

Further, complex languages such as music can be viewed from many abstract
interpretations. One such abstract domain is the \emph{melodic
interval}: it is clear (\todo is it?) that intervalic patterns in music capture
regularity that is \emph{invariant} to the absolute pitch. To take this
abstraction to an extreme, we can consider the melodic \emph{contour}, which we
might represent with the domain $\set{-1,0,1}$, where the elements indicate a
decrease in pitch, stationary pitch, and increase in pitch respectively.
Modelling the regularity in such abstract domains and transforming the
predictions back into predictions over the concrete $\tau_i$ might therefore
further allow the system to generalise.  

\emph{Viewpoints} allow us to model music from these different points of view:
namely, we can model the individual $\tau_i$, as well as abstract
interpretations and combinations thereof. We now proceed to set up the formalism
of multiple viewpoints.

First, allow $\tau$ to range over types other than just the \emph{basic types}
that make up the component types of $\zeta$. We call these non-surface types
\emph{derived types}: the abstract interpretations derived from the surface
$\tau_i$. 

\textbf{Definition}. A \emph{viewpoint} modelling a type $\tau$ is:
\begin{enumerate}[label=\arabic*., itemsep=0mm]
  \item a partial function $\Psi_\tau : \zeta^* \rightharpoonup [\tau]$,
    together with
  \item a context model of sequences in $[\tau]^*$.
\end{enumerate}

The function $\Psi_\tau$ for a type $\tau$ is known as the \emph{projection}
function for $\tau$: it projects out the last element of type $\tau$ from some
surface event stream $e_1^k \in \zeta^*$, if such an element exists. Note that,
for convenience, we shall frequently refer to viewpoints simply by the type they
model. 

Viewpoints, as we have defined them, can clearly model individual surface and
derived types. While this is useful, we also want to be able to capture the
correlation between attributes.

\textbf{Definition}. A \emph{product type} $\tau_1 \otimes \cdots \otimes
\tau_n$ between $n$ types $\tau_1, \ldots, \tau_n$ is itself a type $\tau$ with
$[\tau] = [\tau_1] \times \cdots \times [\tau_n]$. 

For a product type $\tau = \tau_1 \otimes \cdots \otimes \tau_n$:
$$ \Psi_\tau(e_1^k) \triangleq
\begin{cases}
  \langle\Psi_{\tau_1}(e_1^k), \ldots, \Psi_{\tau_n}(e_1^k)\rangle & \forall i
  \in \set{1,\ldots,n}.\
  \Psi_{\tau_i}(e_1^k)\downarrow \\
  \bot & \text{otherwise.}
\end{cases}
$$

Viewpoints over product types are known as \emph{linked viewpoints}. 

Note that one can also make use of \emph{threaded viewpoints}: those defined
only at certain fixed intervals in a sequence. However, these are considered out
of scope for this work.

In order to train a viewpoint over type $\tau$, we need to provide the
underlying context model with sequences in $[\tau]^*$. To obtain such sequences
from surface events, we simply iterate the projection function $\Psi_\tau$ to
give a function $\Phi_\tau : \zeta^* \rightarrow [\tau]^*$ which we call the
\emph{lifting} function, defined as follows:
\begin{align*}
  \Phi_\tau(()) &\triangleq () \\
  \Phi_\tau(e_1^k) &\triangleq \begin{cases}
    \Phi_\tau(e_1^{k-1})::\Psi_\tau(e_1^k) & \Psi_\tau(e_1^k)\downarrow \\
    \Phi_\tau(e_1^{k-1}) & \text{otherwise.}
  \end{cases}
\end{align*}

In practice, we shall see that it is typically easier to implement $\Phi_\tau$
directly. However, formally, it is cleaner to specify $\Psi_\tau$ to define a
particular viewpoint.

Now consider a viewpoint over a type $\tau$ derived from some surface type
$\tau'$. This viewpoint will predict distributions over the abstract type
$\tau$, but we actually need a distribution over the concrete $\tau'$. 

The process of transforming a distribution from the abstract domain to the
concrete is given a limited treatment in the literature. Whorley
\cite{whorley2013phd} notes that this is effectively ``using the partial
function $\Psi_\tau$ in reverse on each of the viewpoint elements''. We argue
that the surface context is also needed to perform this task, and shall refer to
this process as \emph{reification}, specified by a partial function $\rho :
\zeta^* \times \mathrm{dist}(\tau) \rightharpoonup \mathrm{dist}(\tau')$, where
$\mathrm{dist}(\tau)$ denotes the set of probability distributions over a type
$\tau$. This shall be discussed further in Chapter~\ref{chap:impl}.

\subsection{Combining Viewpoint Predictions}\label{sec:vp-comb}

Recall that, given some musical context $e_1^k \in \zeta^*$, the goal is to
predict the next event $e_{k+1} \in \zeta$ where $\zeta = [\tau_1] \times \cdots
\times [\tau_n]$.  A collection of viewpoints that performs this task is known
as a \emph{multiple viewpoint system} (MVS). 

A MVS decomposes this task by predicting $e_{k+1}$ componentwise. Out of all the
viewpoints in the system, consider just those viewpoints capable of predicting
some $\tau_i$. At each timestep, given the context $e_1^k$, $N$ of these
viewpoints will \emph{activate}, meaning they predict a distribution for
$\tau_i$. This section concerns techniques for the combination of $N$ such
distributions to form an overall prediction. In particular, we consider
\emph{weighted} schemes as these are widely used in the literature and known to
perform better than unweighted schemes \cite{pearce2004combining}.

The premise of weighted schemes for viewpoint combination is that viewpoints
that are more `confident' in their predictions should be given higher weights.
Since the Shannon entropy of a viewpoint's distribution is a metric of overall
uncertainty, we use weights that are monotonically non-increasing as a function
of distribution entropy.

Suppose we have $N$ distributions over a type $\tau$. Suppose further that
there is some canonical ordering of $[\tau]$, such that we can pick out the
$j$\textsuperscript{th} member. Now, let $p_i(j)$ denote the probability
assigned to $j$ by the $i$\textsuperscript{th} of the $N$ distributions. We want
to combine these predictions to produce some overall distribution with
probabilities $p(j)$.

\begin{figure}[H]
\centering
\includegraphics[width=400pt]{figs/dist_comb.pdf}
\caption{Plots illustrating effect of distribution combination schemes ($b = 2$)}
\label{fig:dist-comb-plot}
\end{figure}

A general weighted arithmetic scheme combines the predictions as follows:
$$
  p(j) = \frac{ \sum_{i = 1}^N w_i p_i(j) }{ \sum_{i = 1}^N w_i }.
$$

The Shannon entropy of the $i$\textsuperscript{th} distribution is given by:
$$ H(i) = - \sum_{j = 1}^{|[\tau]|} p_i(j) \log_2 p_i(j) $$
with maximum value $H_{\mathrm{max}} = \log_2{ |[\tau]| }$. Now define the
\emph{normalised entropy} $\hat{H}(i)$ as:
$$ \hat{H}(i) \triangleq \begin{cases}
  H(i)/H_{\mathrm{max}} & H_{\mathrm{max}} > 0 \\
  1 & \text{otherwise.}
\end{cases} $$

In previous work this quantity has been called the \emph{relative entropy};
here, we use \emph{normalised entropy} so as to avoid confusion with the
Kullback-Leibler distance, which is also known by this name.

Finally, so that we can favour distributions with lower $\hat{H}$, we introduce
an exponential bias $b \in \mathbb{R}_0^+$:
$$ w_i = \hat{H}(i)^{-b}. $$

Note that we do not restrict $b$ to integer values, as is commonly done in the
literature \cite{whorley2013phd}. Pearce \cite{pearce2004improved} introduces a
new method for combining viewpoint predictions: namely, a \emph{weighted
geometric mean}:
$$ p(j) = \frac{1}{Z} \left( \prod_{i = 1}^N p_i(j)^{w_i} \right)^{ \frac{1}{
\sum_{i = 1}^N w_i }} $$

where $Z$ is a normalisation constant, and the $w_i$ are calculated as before.

Figure~\ref{fig:dist-comb-plot} shows the effect of using these two schemes on
some example distributions. It can be seen that a very low probability
prediction from one distribution has considerably more bearing with the
geometric scheme. Note also that when combining distributions $C$ and $D$, since
$D$ has much lower entropy, it has considerably more effect on the overall
result under both schemes.

\subsection{MVS Architecture}

\begin{figure}[H]
\centering
\includegraphics[width=250pt]{figs/mvs_arch_tmp.jpg}
\caption{Typical MVS architecture}
\label{fig:mvs-arch}
\end{figure}

For successful sequence prediction, it is necessary to capture both
intra-sequence and inter-sequence regularity: that is, the \emph{short-term}
effects \emph{local} to a particular sequence, and the \emph{long-term} effects
common through the entire corpus.  This is especially true of music, and the
chorales in particular, where entire melodic fragments are often re-used within
a given chorale melody.

In a conventional MVS architecture, this is achieved explicitly by using
entirely separate \emph{long-term} and \emph{short-term} models. The long-term
model is trained on the entire corpus, and the short-term model just on the
sequence being predicted or generated. Figure~\ref{fig:mvs-arch} illustrates
this architecture: viewpoint combination is indicated by $\oplus$, and is
performed as per Section~\ref{sec:vp-comb}.

In order to keep the number of hyperparameters under control, the following
architectural constraints are often enforced (and indeed, will be in this work):
\begin{itemize}
  \item The same set of viewpoints $\set{\tau_1, \ldots, \tau_N}$ is used in both
    the long-term and short-term model. 
  \item The viewpoints in the long-term model all have the same order bound
    $h_l$. Similarly, those in the short-term model all have a (possibly
    distinct) order bound $h_s$.
  \item The viewpoint combination within each of these two models is performed
    using the same bias parameter $b_{\mathrm{int}}$ and the combination of the
    short-term and long-term predictions using a separate bias parameter
    $b_{\mathrm{ext}}$.
\end{itemize}

\section{Recurrent Neural Networks}\label{sec:rnn-intro}

Recurrent neural networks can be formulated as a dynamical system with a state
evolving over time. Given an initial state $\vect{h}_0$ and input data
$\vect{x}_t \in \mathbb{R}^m$, the hidden state $\vect{h}_t \in \mathbb{R}^n$ of
a RNN evolves as per (\ref{eq:dyn-sys}) for $t \in \mathbb{N}$.
\begin{equation}
  \vect{h}_t = f(\vect{h}_{t-1}, \vect{x}_t; \vect{\theta})
  \label{eq:dyn-sys}
\end{equation} 

The output
$\vect{o}_t$ may then be a function of the hidden state $\vect{h}_t$, or indeed
a function of both the state $\vect{h}_t$ and input vector $\vect{x}_t$. The key
observation to make from (\ref{eq:dyn-sys}) is that the same parameters
$\vect{\theta}$ are used at each iteration of $f$. 

We can usefully represent such a network as a \emph{computational graph}. Figure
\todo uses this representation, and illustrates the operation of
\emph{unfolding} a recurrence in a computational graph.

\todo Graph unfolding figure here.

In a basic RNN, the choice of $f$ is a simple neural network unit: a
nonlinearity (such as the hyperbolic tangent or logistic sigmoid function)
applied to an affine transformation \cite{zaremba2014recurrent}. Using
$[\vect{u},\vect{v}] \in \mathbb{R}^{m+n}$ to denote the concatenation of
vectors $\vect{u} \in \mathbb{R}^m$ and $\vect{v} \in \mathbb{R}^n$: 
$$f(\vect{h}_{t-1}, \vect{x}_t) = \sigma(\vect{W}[\vect{h}_{t-1}, 
\vect{x}_t] + \vect{b})$$ 
where $\vect{W} \in \mathbb{R}^{(m+n) \times n}$, and $\vect{b} \in
\mathbb{R}^n$. 

The next section explains how we can train recurrent networks in general, and
then proceeds to explore some of the problems due to this simple choice of $f$.

\subsection{Training Recurrent Networks}

The algorithm typically used for training a RNN, known as \emph{backpropagation
through time} (BPTT), applies the standard backpropagation algorithm to the
unfolded computational graph for our RNN. That is, we calculate the derivative
of the cost function with respect to the weights in our network \emph{at each
timestep}. Since the parameters are shared across time, we then sum the gradient
contributions from each timestep and update the weights accordingly by gradient
descent.

Note that training such a network is expensive: supposing we unfold the graph up
to $T$ timesteps, we have to compute the entire forward pass through the network
in $\Theta(T)$ time before gradients can be calculated. Moreover, this cannot be
parallelised, since the computation at timestep $t$ depends on
$\vect{h}_{t-1}$, the result of the computation at the previous timestep.

Even in a recurrent network that is \emph{spatially} shallow (i.e.\ with a
single layer at each timestep), unfolding the computational graph results in a
graph that is very deep: the network itself is \emph{temporally deep}. With the
choice of $f$ in a basic RNN, the repeated composition of an affine
transformation and nonlinearity can exhibit highly nonlinear behaviour. While
this can be desirable, granting the network much expressive power, training such
networks can be problematic. 

\todo Figure illustrating exploding gradients: generate this with a quick numpy
script.

One such problem in deep networks is that the high-dimensional surface of our
loss function with respect to the weights can exhibit sharp ``cliffs'' where the
gradient is very large \cite{Goodfellow-et-al-2016}: this is known as the
\emph{exploding gradient problem}. The negative effects of this problem on
training can, however, largely be mitigated by \emph{clipping} the norm of the
gradient to some maximum value during training.

\subsubsection{Long-term dependencies}

A more insidious problem with basic recurrent networks is the \emph{vanishing
gradient problem}. This is caused not only by the network being very deep, but
in particular by the repeated application of $f$ with the same parameters at
each timestep.

Here, we follow the illustrative treatment of Goodfellow et al.\
\cite{Goodfellow-et-al-2016} and refer to Hochreiter and Schmidhuber
\cite{hochreiter1997long} for a deeper treatment. This argument makes several
simplifying assumptions, but clearly illustrates how the problem might arise in
a simpler setting.

Suppose we simplify the function composition in a neural network to simply apply
matrix multiplication (with no inputs, biases, or nonlinearities). The
recurrence relation: 
$$ \vect{h}_t = \vect{W}^\top \vect{h}_{t-1} $$ 
describes the evolution of such a network. A straightforward induction shows:
$$ \vect{h}_t = (\vect{W}^t)^\top\vect{h}_0 $$
and, supposing that $\vect{W}$ has an eigendecomposition of the form:
$$ \vect{W} = \vect{Q}\vect{\Lambda}\vect{Q}^\top $$
with orthogonal $\vect{Q}$ (i.e.\ $\vect{Q}\vect{Q}^\top = \vect{Q}^\top\vect{Q}
= \vect{I}$), the recurrence can be further simplified to:
$$ \vect{h}_t = (\vect{Q}^\top \vect{\Lambda}^t \vect{Q}) \vect{h}_0. $$
As $t \rightarrow \infty$, any eigenvalues with magnitude less than $1$ will
vanish, and those with magnitude greater than $1$ will explode. The problem of
vanishing and exploding gradients comes from the fact that gradients through the
graph of a such a (simplified) network are \emph{also} scaled according to
$\vect{\Lambda}^t$ \cite{Goodfellow-et-al-2016}.

Complex languages such as music require a good predictor to learn long-term
dependencies in sequences. However, if the gradient vanishes in this manner as
we calculate gradients further back in time, then gradient descent will
experience an exponential slow-up in learning such dependencies. Eventually,
we will also encounter numerical problems with gradients of sufficiently small
magnitude. For this reason, learning long-term dependencies with basic RNNs is
considered intractable.

\subsection{Long Short-Term Memory}

In 1997, Hochreiter and Schmidhuber discovered a radically different RNN
architecture known as \emph{long short-term memory} (LSTM)
\cite{hochreiter1997long}. Despite the superficial complexity of this
architecture, it is based on a simple fundamental idea which is used to achieve
constant gradient flow through time, thereby enabling LSTM networks to learn
long-term dependencies over many timesteps. The idea is to maintain additional
state known as the \emph{cell state} which flows through the network with only
minor \emph{linear}, \emph{pointwise} interactions, protected by structures
known as gates. 

There are in fact many variants of LSTM: we largely follow Zaremba et al.\
\cite{zaremba2014recurrent} in both notational conventions and LSTM design.
Before introducing LSTMs in detail, we shall first define some notation and
introduce the notion of deep recurrent networks in general.

In a deep recurrent network, information is not only processed horizontally
through time, but also vertically through $L$ hidden layers ($L > 1$). We use a
homogeneous state representation with all states in $\mathbb{R}^n$, including
all input and output vectors. 

Let subscripts denote timesteps and superscripts denote layers so that
$\vect{h}_t^l \in \mathbb{R}^n$ denotes the state at time $t$ in layer $l$. In
order to represent input events $x_t$ drawn from an alphabet $\Sigma$ as vectors
in $\mathbb{R}^n$, we use an embedding $E : \Sigma \rightarrow \mathbb{R}^n$
which is either learned through pre-training or learned together with the
weights for the network. Let $\vect{h}_t^0 = E(x_t)$ denote the input vector at
time $t$ and take $\vect{h}_t^L$ as the output vector at time $t$.

We can now specify a RNN with a deterministic transition function from previous
to current states:
$$ \delta : \vect{h}_t^{l-1}, \vect{h}_{t-1}^l \rightarrow \vect{h}_t^l $$
so a basic RNN as described in Section~\ref{sec:rnn-intro} can be specified as:
$$ \delta(\vect{h}_{t-1}^l, \vect{h}_t^{l-1}) = f(\vect{W}^l_x [\vect{h}_{t-1}^l,
\vect{h}_t^{l-1}] + \vect{b}^l_x) $$
where $\vect{W}^l_x \in \mathbb{R}^{2n \times n}$, $\vect{b}^l_x \in
\mathbb{R}^n$, and $f \in \set{ \sigma, \mathrm{tanh} }$ is a nonlinearity.

Typically, in sequence prediction, the output state $\vect{h}_t^L$ at time $t$
is fed into a densely connected layer to obtain an output vector $\vect{o}_t \in
\mathbb{R}^{|\Sigma|}$:
$$ \vect{o_t} = \vect{W}_y \vect{h}_t^L + \vect{b}_y $$
where $\vect{W}_y \in \mathbb{R}^{n \times |\Sigma|}$ and $\vect{b}_y \in
\mathbb{R}^{|\Sigma|}$. 

Treating $\vect{o}_t$ as log-probabilities, we can obtain a probability
distribution over output symbols $y_t \in \Sigma$ with a softmax activation on
this layer:
$$ p(y_t) = \mathrm{softmax}(\vect{o}_t) $$
where for a vector $\vect{z} \in \mathbb{R}^n$, $\mathrm{softmax}(\vect{z})$ is
given by:
$$ \mathrm{softmax}(\vect{z})_i = \frac{ e^{z_i} }{ \sum_{j = 1}^n e^{z_j} }. $$

Figure~\ref{fig:deep-rnn-arch} illustrates the general architecture of a deep
RNN. The merging of arrows denotes \emph{vector concatenation}. We refer to the
units $A_t^l$ as \emph{cells}, specified by the transition function $\delta$.
Note that weight sharing occurs across time, but each layer has its own
parameters. This makes sense since each layer processes data of a (semantically)
different type. The initial state $\vect{h}_{\mathrm{init}}^l$ for the cells in
the $l$\textsuperscript{th} layer is typically set to $\vect{0}$ or initialised
randomly. 

\begin{figure}[H]
\centering
\includegraphics[width=350pt]{figs/deep_rnn_tmp.jpg}
\caption{Flow of information in deep RNN}
\label{fig:deep-rnn-arch}
\end{figure}

We now have the necessary machinery to introduce long short-term memory cells in
the context of deep recurrent networks. LSTM cells replace the $A_t^l$ in the
network with a complex structure centered around carefully protecting the
\emph{cell state}: an additional state vector $\vect{c}_t^l$ that allows data to
flow through many timesteps without undergoing complex non-linear information
morphing as in the case of $\vect{h}_t^l$.

Therefore, our $\delta$ function for LSTMs needs to specify the temporal
transformation of $\vect{c}_t^l$ in addition to the hidden state
transformations:
$$ \delta_{\mathrm{LSTM}} : \vect{h}_{t-1}^l, \vect{h}_t^{l-1}, \vect{c}_{t-1}^l
\rightarrow \vect{h}_t^l, \vect{c}_t^l. $$

The flow of information through an LSTM network is shown in
Figure~\ref{fig:deep-lstm-arch}.

\begin{figure}[H]
\centering
\includegraphics[width=400pt]{figs/lstm_net_tmp.jpg}
\caption{Flow of information in deep LSTM network}
\label{fig:deep-lstm-arch}
\end{figure}

Let $\ast$ denote pointwise vector multiplication. Given input
$\vect{h}_t^{l-1}$, recurrent state $\vect{h}_{t-1}^l$, cell state
$\vect{c}_{t-1}^l$, and writing $\vect{x} = [\vect{h}_{t-1}^l,
\vect{h}_t^{l-1}]$, an LSTM first computes intermediates as per equations
(\ref{eq:lstm-f}) through (\ref{eq:lstm-d}).
\begin{align}
  \vect{f} &= \sigma(\vect{W}_f \vect{x} + \vect{b}_f) \label{eq:lstm-f} \\
  \vect{i} &= \sigma(\vect{W}_i \vect{x} + \vect{b}_i) \\
  \vect{o} &= \sigma(\vect{W}_o \vect{x} + \vect{b}_o) \\
  \vect{d} &= \tanh(\vect{W}_d \vect{x} + \vect{b}_d) \label{eq:lstm-d}
\end{align}
The new states $\vect{c}_t^l$ and $\vect{h}_t^l$ are then computed with
equations (\ref{eq:lstm-c}) and (\ref{eq:lstm-h}). 
\begin{align}
  \vect{c}_t^l &= \vect{f} \ast \vect{c}_{t-1}^l + \vect{i} \ast \vect{d}
  \label{eq:lstm-c} \\
  \vect{h}_t^l &= \vect{o} \ast \tanh(\vect{c}_t^l) \label{eq:lstm-h}
\end{align}

Note that, as before, the parameters are shared across timesteps but not between
layers: layer superscripts are excluded from the parameters in the LSTM
equations for clarity. Figure~\ref{fig:lstm-cell} shows the structure of this
LSTM cell and interprets the LSTM equations graphically.

\begin{figure}[H]
\centering
\includegraphics[width=300pt]{figs/lstm_detail_tmp.png}
\caption{LSTM Cell Structure}
\label{fig:lstm-cell}
\end{figure}

The purpose of the cell state $\vect{c}_t^l$ is to act as a long-term memory.
The LSTM can be understood intuitively as protecting and using this cell state
by following three principles of selectivity:
\begin{enumerate}[label=\arabic*., itemsep=0mm]
  \item \textbf{Write} selectively: only a summary of the information entering
    the cell should be written to the protected memory.
  \item \textbf{Read} selectively: only a summary of the information stored in
    the cell's memory should be included in the output state vector.
  \item \textbf{Forget} selectively: occasionally, previously-useful memories
    will no longer be relevant, and the cell should forget them.
\end{enumerate}

The sigmoidal units $\vect{f}, \vect{i}$, and $\vect{o}$ are known as the
\emph{gates} of the LSTM cell: they are used to scale the activations of other
units componentwise and regulate the flow of information through the cell. The
function of each of the LSTM units can be understood in terms of these
principles as follows:

\begin{itemize}
  \item $\vect{f}$ is known as the \emph{forget gate}: it selectively removes
    information from the previous cell state.
  \item The tanh layer $\vect{d}$ is known as the \emph{input feature}. It
    computes data that we may wish to store in the protected cell state.
\item $\vect{i}$ is known as the \emph{input gate}: it controls which data from
  the input feature gets written into the cell state.
\item Finally, $\vect{o}$ is the \emph{output gate}: it regulates which data
  gets output based on the cell state.
\end{itemize}

Note that, since the introduction of LSTM, many other variants have appeared.
For example, an LSTM that also considers the previous cell state
$\vect{c}_{t-1}^l$ when evaluating the gate units (equations (\ref{eq:lstm-f})
through (\ref{eq:lstm-d})) is known as an LSTM with \emph{peephole} connections.
In addition, there are significant variations such as the \emph{gated recurrent
unit} (GRU). Generally, LSTM variants have only been shown to achieve marginal
performance improvements from the standard LSTM architecture (\todo cite!). We
shall only consider the architecture described thus far.

\subsection{Regularisation}

A known problem with neural networks in general is \emph{overfitting}. For deep
feedforward networks, \emph{dropout} \cite{srivastava2014dropout} has seen
significant success in preventing overfitting (regularisation). It was not until
the recent work of Zaremba et al.\ \cite{zaremba2014recurrent} that it was known
how to successfully apply dropout to recurrent networks. The authors determine
that applying dropout to only the non-recurrent connecitons in the network leads
to successful regularisation. 

\section{Choice of Libraries}

\todo Justify choice of \texttt{music21}, TF, and C++ libraries (JSON/catch)

Initial inspection of relevant corpora and their formats indicated that writing
a parser for an established music notation format such as MusicXML would take a
considerable amount of time and offer limited flexibility. After further
research, it was decided that the Python package \texttt{music21} would be
utilised. \texttt{music21} is an open-source toolkit for computational
musicology. This package was especially useful for the task of corpus
preparation due to the following features:
\begin{itemize}
  \item Built-in corpora including the Bach chorales in MusicXML format.
  \item Parser for various music notation formats, including MusicXML.
  \item High-level features for manipulating musical data.
\end{itemize}

The flexibility offered by \texttt{music21} became of great use later in the
project as the requirements in corpus preparation and analysis changed.

Code for corpus preparation and analysis was written in Python using the
\texttt{music21} package. This code was used by both the MVS and RNN
implementations.

\chapter{Implementation}\label{chap:impl}

\section{Corpus Preparation and Analysis}

Before beginning the implementation of either model, the corpus needed to be
processed and exported to a common format. The high-level description of the
chosen corpus representation in Section~\ref{sec:corp-rep} was made concrete by
using a JSON format to encode the chorales. JSON was chosen due to the
readily-available library support in both Python and \texttt{C++}, as well as
the human-readable nature of the format, so as to simplify the debugging
process. This JSON format was later used not only for the corpus, but also for
musical I/O of both models.

\vspace{4mm}
\begin{minted}[frame=single, linenos=true, fontsize=\footnotesize]{js}
{
  "title": "Aus meines Herzens Grunde (Excerpt)",
  "bwv": "269",
  "time_sig_amt": 12,
  "key_sig_sharps": 1,
  "notes": [
    [67, 8,  4], [67, 12, 8], [74, 20, 4], [71, 24, 6], 
    [69, 30, 2], [67, 32, 4], [67, 36, 6], [69, 42, 2], 
    [71, 44, 4], [69, 48, 8], [71, 56, 4], [74, 60, 8], 
    [72, 68, 4], [71, 72, 4], [69, 76, 8], [67, 84, 8]
  ]
}
\end{minted}

\begin{figure}[H]
\centering
\includegraphics[width=450pt]{figs/aus_meines_excerpt.pdf}
\caption{JSON Chorale excerpt with corresponding score}
\label{fig:chorale-excerpt}
\end{figure}

An example encoding of a chorale melody using this JSON representation is given
in Figure~\ref{fig:chorale-excerpt}. 

Key signatures are represented as integers in $[-7,7]$ where a positive number
indicates the number of sharps, and a negative number the number of flats.
Time signatures are represented as the number of semiquavers in a bar, which
suffices to distinguish \lilyTimeSignature{3}{4} and \lilyTimeSignature{4}{4},
the two time signatures used in the chorales.

In addition to extracting the melodies from the \texttt{music21} choarle corpus
and encoding them in our JSON format, the corpus preparation script performs
some analysis on the corpus to determine the syntactic domains of the various
types of interest in our corpus. That is, the script computes the set of
pitches, durations, intervals, etc.\ that occur in the chorales, and stores
these as metadata along with the corpus. This informed the construction and
representation of musical types in \texttt{C++} classes.

\begin{figure}[H]
\centering
\includegraphics[width=350pt]{figs/key_dist.pdf}
\caption{Graph showing distribution of key signatures in corpus}
\label{fig:key-dist}
\end{figure}

Figure~\ref{fig:key-dist} exemplifies this analysis, in this case considering the
distribution of key signatures in the melodies of our corpus. 

\section{Multiple Viewpoint System}

\subsection{Overview}

A complete framework for implementing Multiple Viewpoint Systems in \verb!C++!
has been implemented. The formalism detailed in Section~\ref{sec:mvs-formalism}
has been fully implemented.  At the core of the framework, the \emph{prediction
by partial match} (PPM) algorithm was implemented for smoothing variable-order
context models. The distributions predicted by these context models can be
combined using the entropy-weighted schemes described in
Section~\ref{sec:vp-comb}.

Abstractions were made such that, using these primitives, \emph{basic},
\emph{derived}, \emph{linked}, and \emph{triply-linked} viewpoints can be
instantiated over arbitrary combinations of types. Both a \emph{long-term}
model, for capturing regularity throughout the corpus, and a \emph{short-term}
model, for capturing the regularity within a composition, were implemented.
Sampling by iterative random walk was implemented for melody generation.
Finally, an automatic viewpoint selection algorithm in the form of \emph{forward
step-wise selection} \cite{pearce2005construction} was implemented.

\begin{figure}[H]
\centering
  \trimbox{0cm 2.5cm 0cm 0cm}{ 
  \begin{tikzpicture}
  \umlclass[type=interface, template={$\zeta$,
    $T_{\text{predict}}$}]{Predictor}{}{
    +predict(context : $\zeta^*$) : dist$\langle T_{\text{predict}}
    \rangle$  \\
    +learn(sequence : $\zeta^*$)
  }
  \umlsimpleclass[y=-4, template={$\zeta$, $T_{\text{viewpoint}}$}]{GeneralViewpoint}
  \umlsimpleclass[x=9, template={$\zeta$, $T_l$, $T_r$}]{GeneralLinkedVP}

  \umlsimpleclass[x=9,y=-4, alias=seqspeclink]{SequenceModel}
  \umlsimpleclass[y=-7, alias=seqspec]{SequenceModel}
  \umlsimpleclass[y=-7,x=9, template={$T$}, alias=seqgen]{SequenceModel}
  \umlsimpleclass[y=-10,x=9, alias=ctxspec]{ContextModel}
  \umlsimpleclass[y=-10, alias=ctxgen, template={$b$}]{ContextModel}
  \umlsimpleclass[y=-13]{TrieNode}
  
  \umlimpl{GeneralViewpoint}{Predictor}
  \umlimpl{GeneralLinkedVP}{Predictor}

  \umlunicompo[arg=model, mult=1]{GeneralViewpoint}{seqspec}
  \umlunicompo[arg=model, mult=1]{GeneralLinkedVP}{seqspeclink}
  \umlreal[stereo={$T \rightarrow T_{\text{viewpoint}}$}]{seqspec}{seqgen}
  \umlreal[stereo={$T \rightarrow \text{Pair}\langle T_l, T_r
  \rangle$}]{seqspeclink}{seqgen}
  
  \umlunicompo[arg=model, mult=1]{seqgen}{ctxspec}
  \umlreal[stereo={$b \rightarrow T::\text{cardinality}$}]{ctxspec}{ctxgen}

  \umlunicompo[arg=trie root, mult=1]{ctxgen}{TrieNode}
  \umlunicompo[arg=children, mult=0..b, recursive=-90|2|3cm]{TrieNode}{TrieNode}
\end{tikzpicture}
}
\hspace{-10mm}
\caption{UML Class Diagram of Prediction Stack}
\label{fig:uml-prediction}
\end{figure}

The implementation achieves much generality by extensive use of \texttt{C++}
\emph{templates}. This is illustrated in Figure~\ref{fig:uml-prediction}: a UML
class diagram outlining the ``prediction stack'': the generic portion of the
viewpoint implementation. As we shall see, specific viewpoints can be
implemented by specialising the template classes \texttt{GeneralViewpoint} and
\texttt{GeneralLinkedVP}. 

The principal design strategy of the implementation was to achieve a symmetry
and tight correspondence between the \texttt{C++} type system and the formalism
of multiple viewpoints detailed in Section~\ref{sec:mvs-formalism}. This was
considered desirable for a number of reasons:
\begin{itemize}
  \item Making full use of the type system in a statically-typed language can
    ensure that many more potential programmer errors are caught at compile time
    than otherwise would be.
  \item A correspondence between the \texttt{C++} type system and the MVS
    formalism provides the basis for a clean API that should be easily
    understood by both \texttt{C++} programmers and those familiar with the
    formalism.
\end{itemize}

\texttt{C++} templates were considered the ideal tool to achieve this. Templates
are \emph{flexible}, allow for considerable \emph{generality}, and are evaluated
at compile-time, achieving better safety guarantees: in particular, they can
eliminate much of the need for dynamic memory allocation, a cause of many common
runtime issues. Although inheritance polymorphism is used to implement an
abstract base class for viewpoints (see Figure~\ref{fig:uml-prediction}),
templates are used as the primary means of achieving polymoprhism.

We shall now give a brief overview of the main components implemented to gain an
appreciation of the intention behind each component and how the overall system
fits together before taking a closer look at various parts of the
implementation.

\begin{itemize}
  \item \texttt{TrieNode<b>} is a generic implementation of the trie data
    structure with branching factor \texttt{b}.
  \item Using this trie implementation, \texttt{ContextModel<b>} implements a
    context model over an alphabet of size \texttt{b}, as per
    Section~\ref{sec:ctx-model-prep}. The inference algorithm is PPM A with
    exclusion.
  \item A simple, unified representation for domain-specific types is used
    throughout.  These are passed as template parameters to many different
    classes. Examples include basic types such as \texttt{ChoralePitch} and
    \texttt{ChoraleDuration}, derived types such as \texttt{ChoraleInterval} and
    abstract meta-types such as \texttt{EventPair<T1,T2>}.
  \item \texttt{SequenceModel<T>} further abstracts \texttt{ContextModel} by
    implementing a higher-level interface to context models in terms of these
    domain-specific types.
  \item \texttt{EventDistribution<T>} represents a probability distribution over
    a type \texttt{T}. It supports random sampling as well as multiple
    distribution combination schemes as per Section~\ref{sec:vp-comb}. Objects
    of this type are returned by \texttt{SequenceModel} objects when predicting
    the next event in a sequence.
  \item All viewpoint implementations derive from the
    \texttt{Predictor<EventSpace,T>} template interface. These are outlined in
    Figure~\ref{fig:uml-prediction} but also include a triply-linked viewpoint
    implementation. All viewpoint objects contain an underlying
    \texttt{SequenceModel} object and support the operations of \emph{lifting}
    and \emph{reification} as per Section~\ref{sec:mvs-formalism}.
  \item \texttt{ChoraleVPLayer} is a container for multiple viewpoints over
    the chorale-specific types, used to
    implement both the \emph{long-term} and \emph{short-term} model.
  \item Finally, \texttt{ChoraleMVS} is a high-level class which manages all the
    components of a multiple viewpoint system over chorales and provides a
    unified interface for tasks such as \emph{prediction} and \emph{generation}.
    Figure~\ref{fig:chorale-uml} illustrates how these components relate to each
    other.
\end{itemize}

\begin{figure}[H]
\centering
  \trimbox{0cm 0.0cm 0cm 0cm}{ 
  \begin{tikzpicture}
  
  \umlclass{ChoraleMVS}{
    - short\_term\_layer : ChoraleVPLayer \\
    - long\_term\_layer : ChoraleVPLayer
  }{
    + add\_viewpoint$\langle T \rangle$(vp : ChoralePredictor$\langle T\rangle$ *) \\
    + predict$\langle T\rangle$(context : vec$\langle$ChoraleEvent$\rangle$) :
    dist$\langle T \rangle$ \\
    + cross\_entropy(seq : vec$\langle$ChoraleEvent$\rangle$) : double \\
    + generate(length : int, ts : TimeSig) : vec$\langle$ChoraleEvent$\rangle$
    \\
    + learn(example : vec$\langle$ChoraleEvent$\rangle$)
  }
  \umlsimpleclass[y=-5]{ChoraleVPLayer}
  \umlsimpleclass[x=-5, y = -9, alias=pitchpred]{Predictor}
  \umlsimpleclass[x=0, y = -9, alias=durpred]{Predictor}
  \umlsimpleclass[x=5, y = -9, alias=restpred]{Predictor}
  \umlsimpleclass[y = -13, type=interface, template={$T$}, alias=cpred]{Predictor}
  \umlsimpleclass[y = -17, type=interface, template={$\zeta$, $T$},
alias=predbase]{Predictor}
  
  \umlunicompo[arg=short-term, mult=1, anchors=-115 and 160]{ChoraleMVS}{ChoraleVPLayer} 
  \umlunicompo[arg=long-term, mult=1, anchors=-65 and 20]{ChoraleMVS}{ChoraleVPLayer}
  \umluniaggreg[arg=pitch\_vps, mult=1..*]{ChoraleVPLayer}{pitchpred}
  \umluniaggreg[arg=duration\_vps, mult=1..*]{ChoraleVPLayer}{durpred}
  \umluniaggreg[arg=rest\_vps, mult=1..*]{ChoraleVPLayer}{restpred}
  \umlreal[stereo={$T \rightarrow$ ChoralePitch}, pos stereo=0.3]{pitchpred}{cpred}
  \umlreal[stereo={$T \rightarrow$ ChoraleDuration}, pos stereo=0.55]{durpred}{cpred}
  \umlreal[stereo={$T \rightarrow$ ChoraleRest}, pos stereo=0.3]{restpred}{cpred}
  \umlreal[stereo={$\zeta \rightarrow$ ChoraleEvent}]{cpred}{predbase}
\end{tikzpicture}
}
\hspace{-10mm}
\caption{UML Class Diagram relating viewpoint framework with Chorale
hierarchy}
\label{fig:chorale-uml}
\end{figure}

\subsection{Context Models}

Recall from Section~\ref{sec:ctx-model-prep} that a context model populates a
database of examples during training and subsequently uses an inference
procedure to perform contextual prediction by matching the context against the
database. We first tackle the task of constructing the underlying data
structure, and then turn to the implementation of PPM.

Given an example sequence $e_1^k$, we need to extract all $h$-grams from this
sequence in order to populate the trie data structure. The procedure for this
can be thought of as passing a sliding window over the input sequence and
generating subsequences. This procedure is detailed in
Algorithm~\ref{alg:sliding-window} and illustrated in
Figure~\ref{fig:hgram-extract} for an input sequence $e_1^5$.

\vspace{4mm}

\begin{algorithm}[H]
  \caption{Sliding window algorithm for $h$-gram extraction}
  \label{alg:sliding-window}
  \begin{algorithmic}[1]
    \Function{learn-sequence}{$e_1^k \in [\tau]^*$}
      \For{$i : 1 \rightarrow k$}
        \State \Call{add-or-increment}{()}
        \For{$j : \min(1,i - \hbar) \rightarrow i$} \Call{add-or-increment}{$e_j^i$}
        \EndFor
      \EndFor
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{figure}[H]
\centering
\includegraphics[width=250pt]{figs/sliding_window_tmp.jpg}
\caption{Illustration of the $h$-gram extraction algorithm with $\hbar = 3$}
\label{fig:hgram-extract}
\end{figure}

The function \Call{add-or-increment}{$e_i^j$} looks up $e_i^j$ in the trie. If
it is found, then its count is incremented. Otherwise, $e_i^j$ is inserted into
the trie, the resulting node initialised with a count of unity. Note that $()$,
the empty sequence, corresponds to the root node in the trie.

While this algorithm can be used to construct a long-term model, a short-term
model requires a context model that can be updated online during sequence
prediction. Context models therefore also need to implement a method which just
extracts those $h$-grams at the tail of the sequence (with the window at the far
right).

\subsubsection{Implementing PPM}

Recall from the discussion in Section~\ref{sec:ctx-model-prep} that considerable
care must be taken to ensure the normalisation of the distributions predcited by
PPM. The technique of \emph{exclusion} maintains state during the trie traversal
in order to reason about the possible context matches at each stage of
execution. Algorithm~\ref{alg:ppm-a} details the PPM A algorithm using this
technique to achieve normalisation.

The key to exclusion in this algorithm is the \textit{Dead} set that
is maintained throughout the recursion. This set should be initialised to
$\varnothing$ and later populated with those events that we \emph{could} have
already matched at a higher-order model and therefore no longer need to consider
at lower-order models. To compute $\mathbb{P}(e' | e_1^k)$ we call
\Call{ppm-a}{$e_1^k$, $e'$, $1$, $\varnothing$}.

\begin{algorithm}[H]
  \caption{PPM A with exclusion}
  \label{alg:ppm-a}
  \begin{algorithmic}[1]
    \Function{ppm-a}{$e_1^k \in [\tau]^*$, $e' \in [\tau]$, $i_{\mathrm{begin}}$, 
    $\textit{Dead} \subseteq [\tau]$}
      \State $\textit{Alive} \gets [\tau] \setminus \textit{Dead}$
      \If{$i_{\mathrm{begin}} > k$}
      \State \Return $1 / |\textit{Alive}|$ \Comment{Base case: uniform
      distribution}
      \EndIf
      \State $i \gets i_{\mathrm{begin}}$
      \For{$i : i_{\mathrm{begin}} \rightarrow k$}
      \If{$C(e_i^k) > 0$} \textbf{break}
      \Comment{Find longest context match}
        \EndIf       
      \EndFor 
      \State $\textit{Known} \gets \set{ e'' \in [\tau]\ |\ C(e_i^k :: e'') > 0 }$
      \State $\textit{Novel} \gets Alive \setminus Known$ \Comment{All
      events we could escape for}
      \State $s \gets \sum_{e'' \in \textit{Alive}} C(e_i^k :: e'')$ 
      \If{$\textit{Novel} = \varnothing$}
        \State \Return $C(e_i^k :: e') / s$ \Comment{No possible need for escape}
      \EndIf
      \If{$e' \in \textit{Known}$}
      \State \Return $C(e_i^k::e') / (1 + s)$ \Comment{Matched $e_i^k::e'$}
      \EndIf
      \State \Return \Call{ppm-a}{$e_1^k$, $e'$, $i+1$, $Dead \cup Known$}$ / (1
      + s)$ \Comment{Escape recursively}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

To realise this pseudocode in \texttt{C++}, the PPM implementation in a
\texttt{ContextModel<b>} instance (with alphabet size \texttt{b}) utilises
bitvectors implemented using \texttt{std::bitset<b>} to efficeintly represent
sets of nodes. In addition, each \texttt{TrieNode<b>} maintains a bitmask
storing which of its children have non-zero counts.
 
\subsection{Event Representation}\label{sec:cpp-event-rep}

In order to abstract the low-level \texttt{ContextModel} implementation and
generally to represent viewpoint types as first-class citizens in \texttt{C++},
a simple scheme was devised to represent viewpoint types as classes.

To interface with the context model implementation, it suffices for each
viewpoint type $\tau$ to specify a well-known enumeration of $[\tau]$. The class
outlined in Listing~\ref{lst:event-rep} is a minimal example of how a class can
conform to this type representation convention.

\begin{listing}[H]
  \begin{minted}[frame=single, linenos=true, fontsize=\footnotesize, mathescape]{cpp}
class T {
  const static cardinality; // set to $|[\tau]|$
  unsigned int encode(); // specifies the enumeration

  T(unsigned int c); // construct using code $0 \leq c < |[\tau]|$
  T(const Other &o); // construct using data 
};
  \end{minted}
  \caption{Prototypical viewpoint type representation}
  \label{lst:event-rep}
\end{listing}

Note that this is nothing more than a convention. Conformance to this convention
is not enforced \emph{explicitly} by any particular language feature. However,
if a class that does not conform to this conveniton is passed to a template
expecting a viewpoint type, this can still be caught at compile time due to
template substitution failure, e.g.\ when a template class attempts to access
\cppi{T::cardinality}.

Many templates in the implementation accept viewpoint types represented in this
way. I also chose to implement an iterator \texttt{EventEnumerator<T>} to
conveniently iterate over the members of a type in the style of the iterators in
the \texttt{C++11} standard template library, enabling clean loops such as~
\cppi{for (auto event : EventEnumerator<T>())}.

\subsection{Sequence Models}

Now that we have established a scheme for representing viewpoint types, we can
abstract context models by providing an interface to them in terms of viewpoint
types. This is the job of a \texttt{SequenceModel}. In particular,
\texttt{SequenceModel<T>} instantiates a \texttt{ContextModel<T::cardinality>}
for \texttt{T} a viewpoint type represented as per
Section~\ref{sec:cpp-event-rep}.

Additionally, given some context, a \texttt{SequenceModel<T>} object can
generate probability distributions over the subsequent event by iterating the
PPM implementaiton of the underlying context model for each member of
\texttt{T}. The object returned from this operation is an instance of
\texttt{EventDistribution<T>}, the functionality of which is discussed
in the following two sections.

\subsection{Distribution Combination}

As mentioned in Section~\ref{sec:vp-comb}, there are various techniques for
combining probability distributions in a mutliple viewpoint system. I initially
implemented an entropy-weighted arithmetic combination scheme, and later, on the
basis of its superior performance \cite{pearce2004combining}, a geometric
scheme.  

In order to cleanly express distribution combination with a standard interface,
yet allow the choice of a different underlying implementation, I utilised the
\emph{Strategy} pattern of object-oriented design
(Figure~\ref{fig:dist-strategy-uml}).

\begin{figure}[H]
\centering
  \trimbox{0cm 0.0cm 0cm 0cm}{ 
  \begin{tikzpicture}
  
  \umlclass[type=interface, template={$T$}, alias=strat]{DistCombStrategy}{}{
  + combine(list$\langle$EventDistribution$\langle T \rangle\rangle$) :
  EventDistribution$\langle T \rangle$
  }
  \umlsimpleclass[y = -4, x=-5, template={$T$}, alias=arith]{ArithmeticComb}
  \umlsimpleclass[y = -4, x=0, template={$T$}, alias=geo]{GeometricComb}
  \umlsimpleclass[y = -4, x=5, template={$T$}, alias=loggeo]{LogGeoComb}
  
  \umlimpl{arith}{strat}
  \umlimpl{geo}{strat}
  \umlimpl{loggeo}{strat}
\end{tikzpicture}
}
\caption{UML Class Diagram illustrating use of Strategy design pattern}
\label{fig:dist-strategy-uml}
\end{figure}

\texttt{ArithmeticComb} is a direct implementation of the weighted arithmetic
formula given in Section~\ref{sec:vp-comb}. \texttt{GeometricComb} is also a
direct implementation of Perace's weighted geometric formula
(\ref{eq:pearce-geometric}) , but this was found to suffer from numerical
instability. 

\begin{equation}
p(j) = \frac{1}{Z} \left( \prod_{i = 1}^N p_i(j)^{w_i} \right)^{ \frac{1}{
\sum_{i = 1}^N w_i }} \label{eq:pearce-geometric}
\end{equation}

Letting $\widetilde{p}(j)$ denote the unnormalised probability for the
$j$\textsuperscript{th} event, we find:
\begin{align*}
  \widetilde{p}(j) &= \left( \prod_{i=1}^N p_i(j)^{w_i}
  \right)^{\frac{1}{\sum_{i=1}^N w_i}} \\[3mm]
  \implies \ln{\widetilde{p}(j)} &= \frac{1}{\sum_{i = 1}^N w_i} \left( \sum_{i
  = 1}^N w_i \ln{p_i(j)} \right) \\[3mm]
  \implies p(j) &= \frac{1}{Z} \exp \left( \frac{\sum_{i = 1}^N w_i \ln{ p_i(j)
  }}{ \sum_{i = 1}^N w_i } \right).
\end{align*}

\texttt{LogGeoComb} implements a geometric combination using this new
expression: the probabilities are first added in log space and then
exponentiated and normalised. This was found to remedy the issues with numerical
stability enitrely.

\subsection{Sampling from Distributions}

In addition to combination, sampling is a central feature of
\texttt{EventDistribution<T>} objects. A high-quality pseudo-random number
generator (PRNG) by the name of \texttt{xoroshiro128+}, the successor to the
well-known \texttt{xorshift128+} PRNG \cite{vigna2017further}, was implemented
in the form of a \texttt{C++11} \texttt{UniformRandomBitGenerator}. This meant
that the algorithm could be used as a random number engine compatible with the
\texttt{C++11} \texttt{<random>} library. As of this writing,
\texttt{xoroshiro128+} has been found to pass more statistical tests than any
other PRNG, including those available in the \texttt{C++11} standard library.

Sampling is performed using the \texttt{xoroshiro128+} implementation by
default, but an alternative random source can easily be configured. The PRNG is
seeded using the system's source of randomness, falling back to the
high-resolution time since boot when this is not available.

\subsection{Early Viewpoints}

\subsection{Linked Viewpoints}

\subsection{Generalised Viewpoints}

\section{Recurrent Neural Network}

A multi-layer LSTM recurrent neural network was implemented in Python using
TensorFlow \cite{abadi2016tensorflow}. The network includes \emph{dropout} for
regularisation as per Zaremba et al.\ \cite{zaremba2014recurrent}. A decoupled
``front-end'' to the RNN was implemented which allows the same internal model to
be used for character-level language modelling and music modelling. Random walk
sampling was implemented for generation. In addition, modifications to the
conventional sequence-prediction RNN architecture were made by feeding the
network additional \emph{global} musical information at each timestep, which was
found to improve performance significantly.

\todo Obviously work out where this should go, but remember that the TF LSTM
implementation adds a \texttt{forget\_bias} of 1 to the biases of the forget
gate because it helps towards the beginning of training. This should be the only
difference with the LSTM architecture presented in the preparation.

\emph{Dropout} \cite{srivastava2014dropout} is a powerful technique for
preventing overfitting in deep feedforward networks which works by randomly
shutting off some fraction of the units in some or all of the layers at train
time. This prevents excessive co-adaption among units in the network.

A recent paper by Zaremba et al.\ \cite{zaremba2014recurrent} made a major
contribution by showing how to apply dropout successfully to recurrent networks.
Although attempts had been made to apply dropout to RNNs prior to this work,
they were largely unsuccessful. In particular, applying dropout to the
\emph{recurrent} connections in a RNN leads to poor performance.

The authors found that applying dropout only to the non-recurrent connections
successfully prevents overfitting in a recurrent network and lead to improved
performance on a number of tasks. In the context of the LSTM formulation of the
previous section, we replace:
$$ \vect{x} = [\vect{h}_{t-1}^l, \vect{h}_t^{l-1}] $$
with
$$ \vect{x} = [\vect{h}_{t-1}^l, \mathrm{D}_p(\vect{h}_t^{l-1})] $$
for equations (\ref{eq:lstm-f}) through (\ref{eq:lstm-d}), where $\mathrm{D}_p$
is the \emph{dropout operator} which randomly sets each component in its result
vector to zero with probability $p$, but otherwise returns its input. 

\subsection{Early Experiments}

\todo Mention language modelling precursor: maybe include snippet of output?

\subsection{Corpus Preprocessing}

\todo Describe the method of \emph{tonal inflation} in order for the RNN to
learn the key as an emergent property of each composition.

\subsection{Domain-specific Features}

\todo Explain ``clock'' inputs. Discuss symmetry with multiple viewpoints.

\section{Testing and Debugging}

\todo Get lots of points and stickers for demonstrating good software
engineering practice by testing thoroughly.

\chapter{Evaluation}\label{chap:eval}


\chapter{Conclusion}

\todo Write me.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\phantomsection
\printbibliography
\addcontentsline{toc}{chapter}{Bibliography}
\todo Check all the references for typos as Google Scholar certainly has a few!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

\todo uncomment me!
%\input{proposal}

\end{document}
