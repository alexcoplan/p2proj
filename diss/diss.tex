% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[UKenglish]{isodate}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{parskip}
\usepackage{verbatim}
\usepackage{color}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage{microtype}
\usepackage{stmaryrd}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lilyglyphs}
\usepackage{fontspec}

\newcommand{\set}[1]{ \left\{ #1 \right\} }
\newcommand{\insharp}[0]{\sharp[raise=0.1,scale=0.8]}
\newcommand{\inflat}[0]{\flat[raise=0.1,scale=0.8]}

\usepackage[style=numeric,backend=biber]{biblatex}
\addbibresource{refs.bib}

\usepackage{docmute}   % only needed to allow inclusion of proposal.tex

\newcommand{\todo}{\textcolor{red}{\textbf{todo}~}}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\cleanlookdateon

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title

\pagestyle{empty}

\rightline{\LARGE \textbf{Alex Coplan}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{A Comparison of Statistical Models and Recurrent Neural Networks for the
Generation of Music} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
St Catharine's College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{r p{10.5cm}}
Name:               & \bf Alex Coplan                       \\
College:            & \bf St Catharine's College                     \\
Project Title:      & \bf A Comparison of Statistical Models and Recurrent
Neural Networks for the \newline Generation of Music \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2017  \\
Word Count:         & \todo\footnote{1} \\
Project Originator: & Alex Coplan \\
Supervisor:         & Matthew Ireland                    \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

The original aim of this project was to implement two models for music
generation and subsequently compare them: namely, a \emph{recurrent neural
network} and \emph{multiple viewpoint system}. The two models were
to be compared using both a listening survey involving human participants and
objective metrics of evaluation, such as information-theoretic measures
of predictive performance.

\section*{Work Completed}

All that has been completed appears in this dissertation.

\section*{Special Difficulties}

\todo
 
\newpage
\section*{Declaration}

I, Alex Coplan of St Catharine's College, being a candidate for Part II of the
Computer Science Tripos, hereby declare that this dissertation and the work
described in it are my own work, unaided except as may be specified below, and
that the dissertation does not contain material that has already been used to
any substantial extent for a comparable purpose.

\bigskip
\leftline{Signed }

\medskip
\leftline{Date }

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

Acknowledge acknowledge acknowledge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}

The modelling and automated generation of music is a central task in an approach
to understanding computational creativity. It is natural to ask whether
computers can produce music that is compelling to humans, and indeed, this
question has long been posed by researchers, with practical efforts dating back
to the mid-1950s \cite{ames1987automated}. 

This dissertation investigates the application of two modern techniques to
\emph{melody generation}, a task highly amenable to statistical
modelling and machine learning. 

An automated system for melody generation is motivated by end-user applications
in \emph{computer-assisted composition}, whereby the system can provide
inspiration for a human composer, either by generating entirely novel melodies
within stylistic constraints, or extemporising on or extending melodic
fragments wrtiten by the human composer. Such a system might augment the
capabilities of typical music notation software. 

Markov modelling is a simple yet effective technique for capturing the
statistics of sequential data. An obvious tool to apply to the modelling of
melody is the Markov chain \cite{ames1989markov}. However, an important
observation to make is that music has a rich underlying structure: modelling the
statistics of notes directly (the \emph{surface structure}) is insufficient to
capture the complex language of musical style. This observation motivated the
development of more sophisticated models, known as \emph{multiple viewpoint
systems} \cite{conklin1995viewpoints}.  A multiple viewpoint system exploits the
rich underlying event structure of complex languages by combining the
predictions of an ensemble of Markov models, each modelling a different
attribute of an event space.

Recurrent neural networks (RNNs), the natural topology of neural network for
modelling sequential data, have been widely applied to tasks such as language
modelling, optical character recognition, time-series modelling and forecasting,
and indeed to the modelling of music (\todo cite). A RNN is an end-to-end
sequence learning tool which relies on very little domain knowledge aside from
the chosen input representation. This means that a system designed for
character-level language modelling can equally be trained to model melody
without changing the network architecture. In practice, however, we shall see
that slight architectural modifications can be of use.

The aim of this work is to compare the performance of multiple viewpoint systems
with that of recurrent neural networks on a task of stylistically-constrained
melody generation. In particular, we assess the predictive performance of the
models on a corpus of melodies used in the chorale harmonisations of J.S.\ Bach.
Furthermore, we compare the sampled outputs from each model by means of a
listening survey involving human participants.

\section{Background}

\subsection{Music Theory}

The development and implementation of the recurrent neural network in this work
can be understood with minimal knowledge of music theory. In general, the
multiple viewpoint formalism can also be expressed independently of any domain
knowledge.  However, to understand the application of the multiple viewpoint
framework to music requires an elementary understanding of music theory.  This
section introduces the relevant terminology to enable such dissussion.

We shall constrain our discussion to the context of Western classical music,
since the style which we wish to model lies within this context. A \emph{pitch}
is an abstract concept which is related to the frequency of a musical note.
Formally, if $p,q$ are pitches with corresponding frequencies $\nu_p,\nu_q$,
then if $\nu_q = 2\nu_p$, $p$ and $q$ are said to span an \emph{octave}, with
$q$ an octave above $p$. In Western classical music, each octave is divided into
twelve distinct \emph{note names} (A through G modified by accidentals
\insharp{} or \inflat{}). A note name, such as `B\inflat' (or, equivalently,
A\insharp), is typically thought of as not just a single pitch, but a
\emph{pitch class}, the set of all pitches an integer number of octaves above or
below any pitch with that note name.  Formally, given some reference frequency
$\nu_n$ for a note name $n$, the set of frequencies of the pitches in the pitch
class for $n$ is given by $\set{ 2^k \cdot \nu_n\ |\ k \in \mathbb{Z} }.$

The mapping between pitch and frequency is known as \emph{temperament}, which on
modern instruments is typically \emph{equal}, meaning that the frequency space is
equally divided among the twelve pitches in an octave. More precisely, the
$(k+1)$\textsuperscript{th} note of the chromatic scale with base frequency $\nu_0$ is
given by $2^{k/12}\cdot\nu_0$. As an aside, all recordings of
model outputs in this work were made using equal temperament. Herein
we will work with the abstraction of pitch, ignoring the underlying frequencies
of notes.

To refer to specific pitches, we make use of \emph{scientific pitch notation}
(SPN). Write $N_k$ where $N$ is a note name and $k$ is an octave number. To
ground all the definitions made thus far, we use the standard reference
frequency $\mathrm{A}_4 \triangleq 440\ \mathrm{Hz}$. An \emph{interval} refers
to the difference in pitch between two notes. Consecutive note names are said to
be a \emph{semi-tone} apart, and an interval of two semi-tones is known as a
\emph{tone}. Later in this work we shall further abstract pitch and intervals
using MIDI pitch numbering, and further concepts shall be defined as necessary.

\todo talk about key/tonality? this section getting quite long \ldots

\subsection{Melody Generation}

\todo Reference literature to explain why this is difficult task.

\section{Work Accomplished}

A complete framework for implementing Multiple Viewpoint Systems in \verb!C++!
has been implemeneted. The formalism as described by Conklin et al.\
\cite{conklin1995viewpoints} has been fully implemented.  At a low level, the
PPM A algorithm \cite{cleary1984ppm} was implemented for smoothing
variable-order context models, which can be parameterised over arbitary types
using \verb!C++!  templates. The distributions predicted by these context models
can be combined using either a \emph{weighted arithmetic} scheme (as per
\cite{conklin1995viewpoints}) or \emph{weighted geometric} scheme, the latter
found to be more effective by Pearce \cite{pearce2005construction} and Whorley
\cite{whorley2013phd}. 

Using these primitives, \emph{primitive}, \emph{derived}, \emph{linked}, and
\emph{triply-linked} viewpoints can be instantiated over arbitrary combinations
of types. Sampling by random walk was implemented for melody generation.
Finally, an automatic viewpoint selection algorithm in the form of \emph{forward
step-wise selection} \cite{whorley2013phd} was implemented.

Code for corpus preparation and analysis was written in Python using the
\texttt{music21} package. This code was used by both the MVS and RNN
implementations.

A multi-layer LSTM recurrent neural network was implemented in Python using
TensorFlow \cite{abadi2016tensorflow}. The network supports \emph{dropout} for
regularisation as per Zaremba et al.\ \cite{zaremba2014recurrent}. A separate
``front-end'' to the RNN was implemented which allows the same internal model to
be used for character-level language modelling and melody modelling. Random walk
sampling was implemented. In addition, modifications to the conventional
sequence-prediction RNN architecture were made by feeding the network additional
\emph{global} musical information at each timestep, which was found to improve
performance significantly.

Finally, a website for human evaluation was developed. This will be discussed in
detail in chapter~\ref{chap:eval}.

\section{Related Work}

The method of multiple viewpoints was first applied to music in 1988 by Conklin
\& Cleary \cite{conklin1988modelling} (no: Ebcioglu??). The authors motivate the use of multiple
viewpoint systems in music by making the observation that it is insufficient to
model the surface structure of music alone.

\section{Structure}

\chapter{Preparation}

\todo

\chapter{Implementation}

\chapter{Evaluation}\label{chap:eval}


\chapter{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\todo
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

\todo uncomment me!
%\input{proposal}

\end{document}
